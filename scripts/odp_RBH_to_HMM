"""
This script takes a RBH file from odp_trio, and performs an HMM search against
all of the other species in the config file.

The output of this is an RBH .tsv file, with or without a group column,
 that has the RBHs and their orthologs.
"""

import ast
from Bio import SeqIO
from Bio import SeqRecord
from Bio import Seq
from itertools import groupby
from itertools import product
import math
import numpy as np
import odp_functions as OdpF
from operator import itemgetter
import pandas as pd
import statistics

configfile: "config.yaml"

config["tool"] = "odp_RBH_to_HMM"

OdpF.check_legality(config)

if not "RBH_file" in config:
    raise IOError("You must specify 'RBH_file' in config, and it must be named just like the output of odp_trio: {species1}_{species_N}_reciprocal_best_hits.tsv")

if not "_reciprocal_best_hits.tsv" in config["RBH_file"]:
    raise IOError("The 'RBH_file' in config must be named just like the output of odp_trio: {species1}_{species_N}_reciprocal_best_hits.tsv")

if not "species" in config:
    raise IOError("You must specify all of the species that will be involved in this analysis in the config.yaml file.")

# make sure none of the sample names have underscores
for thissample in config["species"]:
    if "_" in thissample:
        raise IOError("Sample names can't have '_' char: {}".format(thissample))

# come up with the species in the RBH
myfile = config["RBH_file"]
species_string = myfile.split("/")[-1].replace("_reciprocal_best_hits.tsv", "")
RBH_species = list(sorted(species_string.split("_")))
print("RBH_species")
print(RBH_species)

if len(RBH_species) < 3:
    raise IOError("There must be at least two species in the RBH file.")

# come up with other species to perform the HMM search against
other_species = [x for x in config["species"] if x not in RBH_species]
print()
print("other_species")
print(other_species)

if len(RBH_species) < 1:
    raise IOError("There must be at least one species to search against.")

config["nways"] = len(RBH_species)

# come up with number of RBHs
newstring = "RBH{}way_{}".format(len(RBH_species), "_".join(sorted(RBH_species)))
testdf = pd.read_csv(config["RBH_file"], sep = "\t", index_col = 0)
RBH_entries = list(testdf["RBH"])

rule all:
    input:
        expand(config["tool"] + "/unwrapped_RBH/fasta/unaligned/{RBH}.fasta",
               RBH = RBH_entries),
        expand(config["tool"] + "/unwrapped_RBH/hmm/hmms/{RBH}.hmm",
               RBH = RBH_entries),
        expand(config["tool"] + "/unwrapped_RBH/hmm/searches/{other_species}/{RBH}_against_{other_species}.tsv",
               other_species = other_species, RBH = RBH_entries),
        expand(config["tool"] + "/unwrapped_RBH/hmm/searches_agg/{other_species}_hmm_results.sorted.tsv",
               other_species = other_species, RBH = RBH_entries)
        #expand("unwrapped_RBH/hmm/searches_agg_best/{species}_hmm_best.tsv",
        #       species = config["yaxisspecies"]),

rule generate_fasta_of_each_RBH:
    input:
        RBH = config["RBH_file"],
        proteins = lambda wildcards: [config["species"][x]["proteins"]
                    for x in RBH_species]
    output:
        MBH_fasta = expand(config["tool"] + "/unwrapped_RBH/fasta/unaligned/{RBH}.fasta",
                           RBH = RBH_entries)
    threads: 1
    run:
        print(" - Reading the dataframe of RBHs.")
        df = pd.read_csv(input.RBH, index_col = 0, sep = "\t")

        # make a dict of gene_to_RBH
        print(" - Making a dict of gene_to_RBH")
        species_to_gene_to_RBH = {}
        for index, row in df.iterrows():
            thisRBH = row["RBH"]
            for this_species in RBH_species:
                if this_species not in species_to_gene_to_RBH:
                    species_to_gene_to_RBH[this_species] = {}
                genecol = "{}_gene".format(this_species)
                species_to_gene_to_RBH[this_species][row[genecol]] = thisRBH

        # now make a dict of RBH_to_fasta_records
        print(" - Making a dict of RBH_to_fasta_records.")
        RBH_to_records = {}
        for this_species in RBH_species:
            inhandle = open(config["species"][this_species]["proteins"], "r")
            for record in SeqIO.parse(inhandle, "fasta"):
                if record.id in species_to_gene_to_RBH[this_species]:
                    thisRBH = species_to_gene_to_RBH[this_species][record.id]
                    record.name = record.id
                    newid = "{}_{}".format(thisRBH, this_species)
                    record.id = newid
                    if thisRBH not in RBH_to_records:
                        RBH_to_records[thisRBH] = []
                    RBH_to_records[thisRBH].append(record)

        # make sure that all entries have as many sequences as species
        print(" - Verifying that the records are all complete.")
        for thisRBH in RBH_to_records:
            if len(RBH_to_records[thisRBH]) != len(RBH_species):
                raise IOError("{} only has {} genes".format(
                    thisRBH, len(RBH_to_records[thisRBH])))

        # print out all of the records to fasta files
        print(" - Printing out the FASTA records.")
        for thisRBH in RBH_to_records:
            outfile = config["tool"] + "/unwrapped_RBH/fasta/unaligned/{}.fasta".format(thisRBH)
            with open(outfile, "w") as output_handle:
                for thisrec in RBH_to_records[thisRBH]:
                    SeqIO.write(thisrec, output_handle, "fasta")

rule align_fasta_of_each_group:
    input:
        MBH_fasta = config["tool"] + "/unwrapped_RBH/fasta/unaligned/{RBH}.fasta"
    output:
        aligned =   config["tool"] + "/unwrapped_RBH/fasta/aligned/{RBH}.aligned.fasta",
    threads: 1
    shell:
        """
        mafft --localpair --maxiterate 1000 {input.MBH_fasta} > {output.aligned}
        """

rule make_hmm:
    input:
        aligned = config["tool"] + "/unwrapped_RBH/fasta/aligned/{RBH}.aligned.fasta"
    output:
        hmm     = config["tool"] + "/unwrapped_RBH/hmm/hmms/{RBH}.hmm"
    threads: 1
    shell:
        """
        hmmbuild {output.hmm} {input.aligned}
        """

rule hmm_search_against_genome:
    input:
        proteins = lambda wildcards: config["species"][wildcards.other_species]["proteins"],
        hmm = config["tool"] + "/unwrapped_RBH/hmm/hmms/{RBH}.hmm"
    output:
        tsv = config["tool"] + "/unwrapped_RBH/hmm/searches/{other_species}/{RBH}_against_{other_species}.tsv"
    threads: 2
    shell:
        """
        hmmsearch --tblout {output.tsv} \
          --cpu {threads} \
          --noali \
          --notextw \
          {input.hmm} \
          {input.proteins}
        """

rule aggregate_hmm_results_by_species:
    """
    The header fields are:
    # target name        accession  query name                         accession    E-value  score  bias   E-value  score  bias   exp reg clu  ov env dom rep inc description of target

    the headers for blastp outfmt 6 are:
      qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore
    """
    input:
        tsv = expand(config["tool"] + "/unwrapped_RBH/hmm/searches/{{other_species}}/{RBH}_against_{{other_species}}.tsv",
                     RBH = RBH_entries)
    output:
        tsv =        config["tool"] + "/unwrapped_RBH/hmm/searches_agg/{other_species}_hmm_results.tsv"
    params:
        this_species = lambda wildcards: wildcards.other_species
    threads: 1
    run:
        entries = []
        for thisfile in input.tsv:
            with open(thisfile, "r") as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith("#"):
                        fields = line.split()
                        thisentry = {"qseqid": fields[2].replace(".aligned", ""),
                                     "sseqid": fields[0],
                                     "pident": 50.0,
                                     "length": 50.0,
                                     "mismatch": 0,
                                     "gapopen": 0,
                                     "qstart": 0,
                                     "qend": 0,
                                     "sstart": 0,
                                     "send": 0,
                                     "evalue": float(fields[4]),
                                     "bitscore": float(fields[5]) }
                        entries.append(thisentry)
        df = pd.DataFrame(entries)
        df.to_csv(output.tsv, sep="\t", index = False, header = False)

rule sort_the_collated_hmm_results:
    input:
        tsv =        config["tool"] + "/unwrapped_RBH/hmm/searches_agg/{other_species}_hmm_results.tsv"
    output:
        tsv =        config["tool"] + "/unwrapped_RBH/hmm/searches_agg/{other_species}_hmm_results.sorted.tsv"
    params:
        this_species = lambda wildcards: wildcards.other_species
    shell:
        """
        cat {input.tsv} | sort -k12,12nr > {output.tsv}
        """

#rule best_gene_for_each_hmm:
#    """
#    finds the best hmm for each gene
#    """
#    input:
#        tsv = "unwrapped_RBH/hmm/searches_agg/{species}_hmm_results.tsv",
#        chrom = lambda wildcards: config["xaxisspecies"][wildcards.species]["prot_to_loc"],
#        RBH = "unwrapped_RBH/RBH/reciprocal_best_hits.tsv"
#    output:
#        tsv = "unwrapped_RBH/hmm/searches_agg_best/{species}_hmm_best.tsv",
#        RBHinfo = "unwrapped_RBH/hmm/RBH_plus_HMM_agg_best/{species}_hmm_chrom_info.tsv",
#        groupby = "unwrapped_RBH/hmm/RBH_plus_HMM_agg_best/{species}_hmm_chrom_info.groupby.tsv"
#    params:
#        species = lambda wildcards: wildcards.species
#    threads: 1
#    run:
#        species_gene_to_chrom = {}
#        species_gene_to_pos = {}
#        with open(input.chrom, "r") as f:
#            for line in f:
#                line = line.strip()
#                if line:
#                    fields = line.split()
#                    species_gene_to_chrom[fields[0]] = fields[1]
#                    species_gene_to_pos[fields[0]] = int(fields[3])
#        df = pd.read_csv(input.tsv, index_col = 0, sep = "\t")
#        df = df.sort_values(["sseqid", "evalue"], ascending=[True, True])
#        df = df.drop_duplicates(subset = ["sseqid"])
#        df = df.sort_values(["qseqid", "evalue"], ascending=[True, True])
#        df = df.drop_duplicates(subset = ["qseqid"])
#        df = df.reset_index(drop=True)
#        df.to_csv(output.tsv, header = False, index = False, sep="\t")
#        df = df.iloc[:, 0:2]
#        df.columns = ["RBH", params.species]
#        RBH = pd.read_csv(input.RBH, index_col = 0, sep = "\t")
#        RBH["{}_gene".format(params.species)] = RBH["RBH"].map(
#                    dict(zip(df["RBH"],
#                             df[params.species])) )
#        RBH["{}_scaf".format(params.species)] = RBH[
#            "{}_gene".format(params.species)].map(
#            species_gene_to_chrom)
#        RBH["{}_pos".format(params.species)] = RBH[
#            "{}_gene".format(params.species)].map(
#            species_gene_to_pos)
#        RBH.to_csv(output.RBHinfo,  sep="\t")
#
#        groupbycols = [x for x in RBH.columns if "_scaf" in x]
#        grouped_multiple = RBH.groupby(groupbycols).agg(list).reset_index()
#        # get the size
#        grouped_multiple["count"] = grouped_multiple.RBH.str.len()
#        grouped_multiple.to_csv(output.groupby,  sep="\t")


#def blast_plot_order_helper(coords, sample, xory, xprottoloc, yprottoloc, recip,
#                            xorder):
#    """
#    This uses the reciprocal blast results to come up with the sort order
#     for the y-axis scaffolds. Returns a list of the plot order.
#
#    This code is all duplicated from the synteny plot function.
#     Could be programmed in a better way to avoid redundancy, but this just fits
#     the edge case where the y-axis has to be arranged based on the blast results.
#    """
#    # now make a lookup table of where the prots are.
#    #  Use the x_offset and y_offset to recalculate where the plotting
#    #  value is
#    xcoords = generate_coord_structs_from_chrom_to_loc(xprottoloc)
#    ycoords = generate_coord_structs_from_chrom_to_loc(yprottoloc)
#
#    # now open the blast results and translate the pairs
#    #  into plotting positions
#    df = pd.read_csv(recip, header=None, sep = "\t")
#    df.columns = ["xgene", "ygene", "pident", "length",
#                  "mismatch", "gapopen", "qstart", "qend",
#                  "sstart", "send", "evalue", "bitscore"]
#    df = df[["xgene", "ygene", "bitscore", "evalue"]]
#
#    #print(x_prot_to_loc)
#    df["xpos"] = df["xgene"].map(xcoords["prot_to_middle"])
#    df["ypos"] = df["ygene"].map(ycoords["prot_to_middle"])
#
#    df["xscaf"] = df["xgene"].map(xcoords["prot_to_scaf"])
#    df["yscaf"] = df["ygene"].map(xcoords["prot_to_scaf"])
#    df = df.dropna()
#    df = df.sort_values(by=['xpos'])
#    df = df.dropna()
#
#    grouped_df = df.groupby(["yscaf"])
#    for key, item in grouped_df:
#        max_item = grouped_df.get_group(key)['xscaf'].value_counts().idxmax()
#        all_other_things = [x for x in grouped_df.get_group(key)['xscaf'].unique() if x != max_item]
#        for thisthing in all_other_things:
#            df = df.loc[~( (df["yscaf"] == key) & (df["xscaf"] == thisthing)), ]
#    # now sort based on the xscafs and the xpos
#    sorterIndex = dict(zip(xorder, range(len(xorder))))
#    df.sort_values(['yscaf', 'ypos'],
#        ascending = [True, True], inplace = True)
#    df.reset_index(drop=True, inplace = True)
#    df = df.drop_duplicates(subset=['yscaf'])
#    df['x_Rank'] = df['xscaf'].map(sorterIndex)
#    df.sort_values(['x_Rank', 'xpos'],
#        ascending = [True, True], inplace = True)
#    df = df.dropna()
#    df.reset_index(drop=True, inplace = True)
#    #print(list(df.yscaf))
#    return(list(df.yscaf))
#
#def parse_coords(coords_file, sample, xory,
#                 xprottoloc=None, yprottoloc=None,
#                 recip=None, xorder=None, dropNone = False):
#    """
#    This parses the coordinates and returns a
#      - coord-to-offset dict (I don't remember what this is for),
#      - the size of each scaffold (a dictionary)
#      - a list of locations to plot lines (These are the scaf/chrom divisions)
#      - the max value for that axis
#      - the tick labels
#      - the tick positions
#      - the yorder or xorder
#    """
#    offset = {}
#    max_coord = 0
#    lines_at = []
#    df = pd.read_csv(coords_file, header = None, sep = " ")
#    df.columns = ["scaf", "scaflen", "cumsum", "coordstart"]
#    # now figure out if we need to sort or not
#    drop_nas = True
#    if dropNone:
#        df = df.loc[df["scaf"] != "None", ]
#    # make sure that plotorder and sort_by_x_coord_blast aren't there together
#    #  These are two conflicting sort order operations.
#    #  Specifically, sort_by_x_coord_blast will mess up plotorder.
#
#    if xory == "x":
#        plotorder = None
#    elif xory == "y":
#        #print("we're in y")
#        if sample not in config["yaxisspecies"]:
#            raise IOError("Can't find this yspecies")
#        else:
#            #print("we're in the else of y")
#            if "plotorder" in config["yaxisspecies"][sample]:
#                #print("we're in the plotorder of y")
#                plotorder = config["yaxisspecies"][sample]["plotorder"]
#                #print("plotorder zeroth " + plotorder[0])
#                drop_nas = False
#            elif "sort_by_x_coord_blast" in config["yaxisspecies"][sample]:
#                #print("we're in the sort_by_x_coord_blast of y")
#                if config["yaxisspecies"][sample]["sort_by_x_coord_blast"]:
#                    #print("we're in the sort_by_x_coord_blast of y True")
#                    # we need to set up the sort order based on the occurrence in the blast results
#                    plotorder = blast_plot_order_helper(coords_file, sample, xory,
#                                                        xprottoloc, yprottoloc,
#                                                        recip, xorder)
#                    #print("after plot order in sort_by_x_coord_blast")
#            else:
#                #print("in the else")
#                plotorder = None
#    else:
#        raise IOError("Don't know what this is")
#    #print("df after xory")
#    # now we have determined if we need to sort
#    if plotorder != None: # if plotorder has something in it.
#        #print(" - using custom plot order: ", plotorder)
#        sortdict = {key: val for key, val in zip(plotorder, range(len(plotorder)))}
#        df['rank'] = df['scaf'].map(sortdict)
#        df.sort_values(by = 'rank' ,inplace=True)
#    #print("df after plotorder")
#    #print(df)
#
#    # now, if we made plotorder from config then drop rows
#    if xory == "y":
#        if "plotorder" in config["{}axisspecies".format(xory)][sample]:
#            df = df[df["scaf"].isin(config["{}axisspecies".format(xory)][sample]["plotorder"])]
#    # only drop if we haven't specified the plot order in the config
#    #if drop_nas:
#    #    df = df.dropna()
#    df.reset_index(drop=True, inplace = True)
#    df["cumsum"] = df["scaflen"].cumsum()
#    df["cumsum"] = df["cumsum"] - df["scaflen"]
#    print("df after cumulative sum and sorting")
#    print(df)
#    for i, row in df.iterrows():
#        offset[row["scaf"]] = row["cumsum"]
#        if i > 0:
#            lines_at.append(row["cumsum"])
#    
#    max_coord = list(df["scaflen"].cumsum())[-1]
#
#    #tick labels
#    tick_labels = list(df["scaf"])
#    tick_pos    = list(df["cumsum"] + (df["scaflen"]/2))
#
#    scaf_to_len = {}
#    for i, row in df.iterrows():
#        scaf_to_len[row["scaf"]] = row["scaflen"]
#
#    return (offset, scaf_to_len, lines_at, max_coord, tick_labels, tick_pos, list(df["scaf"]))
#
#
#def calc_D_for_y_and_x(df, x_offset, y_offset, x_scaf_to_len, y_scaf_to_len):
#    """
#    This calculates D for both the x and y axes.
#    Defined in the 2020 vertebrate synteny paper.
#    """
#    df = df.dropna()
#    # some variable names in this for loop are "x" but it doesn't matter.
#    #  everything important is variable between x and y
#    for thisdir in ["x", "y"]:
#        df = df.sort_values(by=["{}middle".format(thisdir)])
#        df.reset_index(drop=True, inplace = True)
#
#        unique_x = df["{}scaf".format(thisdir)].unique()
#        thisdir_dfs = []
#        # this just calculates Dx
#        for thisx in unique_x:
#            xdf = df.loc[df["{}scaf".format(thisdir)] == thisx, ].copy()
#            xdf = xdf.reset_index(drop=True)
#            oppositexy = "d"
#            this_offset = {}
#            this_scaf_to_len = {}
#            if thisdir == "x":
#                oppositexy = "y"
#                this_offset = x_offset
#                this_scaf_to_len = x_scaf_to_len
#            elif thisdir == "y":
#                oppositexy = "x"
#                this_offset = y_offset
#                this_scaf_to_len = y_scaf_to_len
#            df2 = pd.get_dummies(xdf["{}scaf".format(oppositexy)])
#            df2_xiL = df2.apply(lambda x: x.rolling(20).mean(), axis = 0)
#            df2_xiR = df2.apply(lambda x: x.iloc[::-1].rolling(20).mean(), axis = 0).iloc[::-1]
#            df2_xiR = df2_xiR.set_index(df2_xiR.index - 1)
#            df2_xiR = df2_xiR.iloc[1:]
#            subtractdf = df2_xiR.fillna(0) - df2_xiL.fillna(0)
#            D = subtractdf.apply(lambda x: np.sqrt(np.square(x).sum()), axis = 1)
#            xdf["D{}".format(thisdir)] = D
#            xdf["D{}_barleft".format(thisdir)] = 0
#            xdf["D{}_barmiddle".format(thisdir)] = 0
#            xdf["D{}_barright".format(thisdir)] = 0
#            xdf["D{}_barwidth".format(thisdir)] = 0
#            for i, row in xdf.iterrows():
#                barleft   = -1
#                barright  = -1
#                barmiddle = -1
#                barwidth  = -1
#                if len(xdf) > 1:
#                    if i == 0:
#                        thisend   = row["{}stop".format(thisdir)]
#                        nextstart = xdf.loc[i+1, "{}start".format(thisdir)]
#                        barleft   = this_offset[thisx]
#                        barright  = thisend + ((nextstart-thisend)/2)
#                    elif i == (len(xdf) - 1):
#                        prevend   = xdf.loc[i-1, "{}stop".format(thisdir)]
#                        thisstart = row["{}start".format(thisdir)]
#                        barleft   = prevend + ((thisstart-prevend)/2)
#                        barright  = this_scaf_to_len[thisx]
#                    else:
#                        prevend   = xdf.loc[i-1, "{}stop".format(thisdir)]
#                        thisstart = row["{}start".format(thisdir)]
#                        thisend   = row["{}stop".format(thisdir)]
#                        nextstart = xdf.loc[i+1, "{}start".format(thisdir)]
#                        barleft   = prevend + ((thisstart-prevend)/2)
#                        barright  = thisend + ((nextstart-thisend)/2)
#                xdf.loc[i, "D{}_barleft".format(thisdir)]   = barleft
#                xdf.loc[i, "D{}_barright".format(thisdir)]  = barright
#                xdf.loc[i, "D{}_barmiddle".format(thisdir)] = barleft + ((barright - barleft)/2)
#                xdf.loc[i, "D{}_barwidth".format(thisdir)]  = barright - barleft + 1
#            thisdir_dfs.append(xdf)
#        df = pd.concat(thisdir_dfs)
#    df = df.sort_values(by=["xmiddle"])
#    df.reset_index(drop=True, inplace = True)
#    return df
#
#def determine_breaks(df, scaf_to_breaks_set, scaf_to_offset_dict,
#                     sort_direction, auto_breaks):
#    """
#    determines the major breaks in Dx or Dy to use as partitions.
#
#    The input parameters are:
#      - df: the analysis df at the end of synteny plot.
#      - sort_direction: either "x" or "y"
#
#    The output of this method is a dataframe that is just the rows of the input
#     that are the breakpoints in the input df.
#    """
#    # MAGIC NUMBERS
#    # set window to change how many genes on either side are considered when
#    #  looking for peaks. A value of 20 means 20 on either side, so 41 genes total
#    window = 20
#    smallwindow = 5
#    # set small_window to resolve nearby peaks from different datasources
#
#    sort_order = {"x": {"pos": "xmiddle",
#                        "end": "xstop",
#                        "chrom": "xscaf",
#                        "D": "Dx"},
#                  "y": {"pos": "ymiddle",
#                        "end": "ystop",
#                        "chrom": "yscaf",
#                        "D": "Dy"}}
#
#    # sort the dataframe based on which axis we're looking at
#    df = df.sort_values(by=[sort_order[sort_direction]["pos"]])
#    df = df.reset_index(drop=True)
#
#    # first, figure out the manual break positions in terms of the protein coordinates
#    manual_breaks_indices = set()
#    for thisscaf in scaf_to_breaks_set:
#        for thisposition in scaf_to_breaks_set[thisscaf]:
#            offset_position = thisposition + scaf_to_offset_dict[thisscaf]
#            subdf = df.loc[df[sort_order[sort_direction]["end"]] <= offset_position, ]
#            subdf = subdf.sort_values(by=[sort_order[sort_direction]["end"]])
#            tempdf = df.loc[df[sort_order[sort_direction]["chrom"]] == thisscaf, ["xgene", "ygene", "xstart"]]
#            #print(thisscaf, thisposition, offset_position)
#            #print(tempdf)
#            #print(subdf.loc[subdf.index[-1]])
#            #print()
#            #sys.exit()
#            manual_breaks_indices.add(subdf.index[-1])
#    manual_breaks_indices = list(manual_breaks_indices)
#    #if not auto_breaks:
#    #    print("manual_breaks")
#    #    print(manual_breaks_indices)
#
#    all_ranges = set()
#    if auto_breaks:
#        unique_chroms = []
#        #chrom_breakpoints = []
#        for index, row in df.iterrows():
#            thischrom = row[sort_order[sort_direction]["chrom"]]
#            thispos  = row[sort_order[sort_direction]["pos"]]
#            if thischrom not in unique_chroms:
#                unique_chroms.append(thischrom)
#                #chrom_breakpoints.append(thispos)
#        ## this line is solely for plotting. Not useful
#        #chrom_breakpoints = chrom_breakpoints[1::]
#
#        # There are three different analysis types that we will use to figure
#        #  out the seps.
#        # - deltMA is the derivative of the smoothed data
#        # - deltD is the derivative of the raw data
#        # - Dx2 is the raw D data.
#        #
#        # All of the data are selected based on the max value of what was above
#        #  the median for that chromosome.
#        for thiscol in ["MA", "deltMA","deltD", "Dx2"]:
#            for thischrom in unique_chroms:
#                # use .copy() to make sure we're not modifying the original df
#                subdf = df.loc[df[sort_order[sort_direction]["chrom"]] == thischrom, ].copy()
#                # Dx2 is just the raw data that is above the median
#                subdf["Dx2"] = subdf[sort_order[sort_direction]["D"]]
#                subdf['Dx2'] = np.where((subdf[sort_order[sort_direction]["D"]] < subdf[sort_order[sort_direction]["D"]].median()),np.NaN,subdf["Dx2"])
#                # MA is the moving average of the raw data
#                subdf["MA"] = subdf["Dx2"].rolling(window=3, center=True).mean()
#                subdf["MA2"] = subdf["Dx2"].rolling(window=19, center=True).mean()
#                # deltMA is the derivative of the moving average
#                subdf["deltMA"] = subdf["MA"].diff() / subdf["MA"].index.to_series().diff()
#                subdf['deltMA'] = np.where((subdf["MA"] < subdf["MA"].median()),np.NaN,subdf["deltMA"])
#                # deltD is the derivative of the raw data
#                subdf["deltD"] = subdf["Dx2"].diff() / subdf["Dx2"].index.to_series().diff()
#                subdf['deltD'] = np.where((subdf.Dx2 < subdf["Dx2"].median()),np.NaN,subdf.deltD)
#
#                # get the groups of consecutive values in each category
#                idxmaxes = set()
#                ind = list(subdf[~subdf[thiscol].isnull()].index)
#                ranges =[]
#                for k,g in groupby(enumerate(ind),lambda x:x[0]-x[1]):
#                    group = (map(itemgetter(1),g))
#                    group = list(map(int,group))
#                    ranges.append((group[0],group[-1]))
#
#                # now get the peak from each contiguous range of values
#                if len(ranges) > 0:
#                    for this_range in ranges:
#                        if this_range[0] != this_range[-1]:
#                            #this_range = [x for x in range(this_range[0], this_range[1]+1)]
#                            this_range = list(this_range)
#                            which_d_col = sort_order[sort_direction]["D"]
#                            temp = subdf.loc[this_range[0]:this_range[-1]][which_d_col].idxmax()
#                            idxmaxes.add(temp)
#
#                # picks the best in large windows of genes.
#                #  See the description for the 'window' variable above
#                keep_idx_maxes = set()
#                ignore_set = set()
#                done = False
#                consider_ranges = set()
#                while not done:
#                    consider_ranges = set()
#                    # get peaks within the window if they're not in the ignore set
#                    for this_idx in idxmaxes:
#                        thistup = tuple([x for x in idxmaxes
#                             if ((x > this_idx - window)
#                                 and (x < this_idx + window)
#                                 and (x not in ignore_set))])
#                        if len(thistup) > 0:
#                            consider_ranges.add(thistup)
#
#                    # now for each set of peaks, get the best in each window
#                    consider_ranges = sorted(list(consider_ranges), key=len, reverse=True)
#                    if len(consider_ranges) > 0: # skip the empty ranges
#                        thisrange = list(consider_ranges[0])
#                        if len(thisrange) == 1:
#                            done = True
#                        else:
#                            submax = df.loc[thisrange, ][sort_order[sort_direction]["D"]].idxmax()
#                            for thisid in thisrange:
#                                if thisid != submax:
#                                    ignore_set.add(thisid)
#                    else: # if it is empty, leave
#                        done = True
#                # We found the biggest peaks in the windows, add them to all_ranges
#                for entry in consider_ranges:
#                    all_ranges.add(entry)
#
#    # flatten the results of what we got from the last analysis
#    idxmaxes = OdpF.flatten(all_ranges)
#    idxmaxes = OdpF.flatten([idxmaxes, manual_breaks_indices])
#
#    # From the dataset of all peaks, find the best in small windows.
#    #  See the variable `smallwindow` above
#    # The reason we have this block is that the same peak, or something near it,
#    #  could have been added multiple times, at slightly different indices.
#    #  This collapses the similar indices to get the best.
#    ignore_set = set()
#    done = False
#    consider_ranges = set()
#    while not done:
#        consider_ranges = set()
#        for this_idx in idxmaxes:
#            # get windows of ranges if they're not in the ignore set
#            thistup = tuple([x for x in idxmaxes
#                 if ((x > this_idx - smallwindow)
#                     and (x < this_idx + smallwindow)
#                     and (x not in ignore_set))])
#            if len(thistup) > 0:
#                consider_ranges.add(thistup)
#
#        consider_ranges = sorted(list(consider_ranges), key=len, reverse=True)
#        if len(consider_ranges) > 0:
#            thisrange = list(consider_ranges[0])
#            if len(thisrange) == 1:
#                done = True
#            else:
#                submax = df.loc[thisrange, ][sort_order[sort_direction]["D"]].idxmax()
#                for thisid in thisrange:
#                    if thisid != submax:
#                        ignore_set.add(thisid)
#        else:
#            # there's nothing here
#            done = True
#
#    # vert_lines is the list of indices from the df that have the peaks that
#    #  we want to keep.
#    vert_lines = OdpF.flatten(consider_ranges)
#    # return a dataframe of the intersections we want
#    return df.loc[vert_lines].copy()
#
#def gen_plotting_df(ycoords_file, xcoords_file,
#                    xprottoloc, yprottoloc,
#                    xsample, ysample,
#                    recip, outtable, plotorder_file):
#    """
#    Generates a dataframe that will be used by the other parts of the program
#     for plotting.
#    Saves it to a file
#    """
#    import pandas as pd
#    import numpy as np
#    # set seaborn stuff
#
#    # first make a lookup table of how to calculate the
#    #  x and y coords_file. This lookup is just the amount of
#    # bp to add to the value when plotting. We pass the xprot_to_loc,
#    #  xprot_to_scaf in case we need to sort everything based on order of
#    #  occurrence on the scaffolds
#    x_offset, x_scaf_to_len, vertical_lines_at, xmax, xticklabel, xtickpos, xorder = parse_coords(
#        xcoords_file, xsample, "x")
#    print("found {} x chromosomes".format(len(x_offset)))
#
#    y_offset, y_scaf_to_len, horizontal_lines_at, ymax, yticklabel, ytickpos, yorder = parse_coords(
#        ycoords_file, ysample, "y",
#        xprottoloc, yprottoloc, recip, xticklabel)
#    print("found {} y chromosomes".format(len(y_offset)))
#
#    # now save the plot order to a file
#    with open(plotorder_file, "w") as f:
#        print("xplotorder:", file=f)
#        for entry in xorder:
#            print("  - {}".format(entry), file=f)
#        print("yplotorder:", file=f)
#        for entry in yorder:
#            print("  - {}".format(entry), file=f)
#
#    # now make a lookup table of where the prots are.
#    #  Use the x_offset and y_offset to recalculate where the plotting
#    #  value is
#    xstruct = generate_coord_structs_from_chrom_to_loc(xprottoloc)
#    xstruct["prot_plot_start"] = {}
#    xstruct["prot_plot_middle"] = {}
#    xstruct["prot_plot_stop"] = {}
#    ystruct = generate_coord_structs_from_chrom_to_loc(yprottoloc)
#    ystruct["prot_plot_start"] = {}
#    ystruct["prot_plot_middle"] = {}
#    ystruct["prot_plot_stop"] = {}
#    #print("xstruct")
#    #print(xstruct)
#    # get rid of proteins that we don't need along the x axis
#    #  also set the plotting position based on the offset
#    for prot in list(xstruct["prot_to_middle"].keys()):
#        scaf = xstruct["prot_to_scaf"][prot]
#        if scaf not in x_offset:
#            for thisdict in xstruct:
#                xstruct[thisdict].pop(prot, None)
#        else:
#            xstruct["prot_plot_start"][prot]  = xstruct["prot_to_start"][prot] + x_offset[scaf]
#            xstruct["prot_plot_middle"][prot] = xstruct["prot_to_middle"][prot] + x_offset[scaf]
#            xstruct["prot_plot_stop"][prot]   = xstruct["prot_to_stop"][prot] + x_offset[scaf]
#    # get rid of proteins that we don't need along the y axis
#    #  also set the plotting position based on the offset
#    for prot in list(ystruct["prot_to_middle"].keys()):
#        scaf = ystruct["prot_to_scaf"][prot]
#        if scaf not in y_offset:
#            for thisdict in xstruct:
#                ystruct[thisdict].pop(prot, None)
#        else:
#            ystruct["prot_plot_start"][prot]  = ystruct["prot_to_start"][prot] + y_offset[scaf]
#            ystruct["prot_plot_middle"][prot] = ystruct["prot_to_middle"][prot] + y_offset[scaf]
#            ystruct["prot_plot_stop"][prot]   = ystruct["prot_to_stop"][prot] + y_offset[scaf]
#
#    # now open the blast results and translate the pairs
#    #  into plotting positions
#    df = pd.read_csv(recip, header=None, sep = "\t")
#    df.columns = ["xgene", "ygene", "pident", "length",
#                  "mismatch", "gapopen", "qstart", "qend",
#                  "sstart", "send", "evalue", "bitscore"]
#    df = df[["xgene", "ygene", "bitscore", "evalue"]]
#    df["xgene"] = df["xgene"].astype(str)
#    df["ygene"] = df["ygene"].astype(str)
#    #print(x_prot_to_loc)
#    df["xstart"]  = df["xgene"].map(xstruct["prot_plot_start"])
#    df["xmiddle"] = df["xgene"].map(xstruct["prot_plot_middle"])
#    df["xstop"]   = df["xgene"].map(xstruct["prot_plot_stop"])
#
#    df["ystart"]  = df["ygene"].map(ystruct["prot_plot_start"])
#    df["ymiddle"] = df["ygene"].map(ystruct["prot_plot_middle"])
#    df["ystop"]   = df["ygene"].map(ystruct["prot_plot_stop"])
#    print(df)
#
#    # I don't remember why the x and y is switched here.
#    df["yscaf"] = df["ygene"].map(ystruct["prot_to_scaf"])
#    df["xscaf"] = df["xgene"].map(xstruct["prot_to_scaf"])
#    #df = df.dropna() # this messed up plotting order if there were no hits on that scaf.
#    df = df.sort_values(by=['xmiddle'])
#    #df = df.loc[df["evalue"] <= float("1E-20"), ]
#    df.reset_index(drop=True, inplace = True)
#
#    # Now calculate D and Dw for both X and Y axes
#    for thiscol in ['Dx', 'Dx_barleft',
#       'Dx_barmiddle', 'Dx_barright', 'Dx_barwidth', 'Dy', 'Dy_barleft',
#       'Dy_barmiddle', 'Dy_barright', 'Dy_barwidth']:
#        df[thiscol] = 0
#    if outtable:
#        df.to_csv(outtable, sep="\t")
#
#rule generate_plotting_df_and_plot_order:
#    """
#    This parses all of the various types of input and generates the df that
#     contains the plotting information like genes, reciprocal-best orthologs.
#
#    Also outputs a file of the plotting order.
#    """
#    input:
#        ycoords = "synteny_analysis/genome_coords/y_genome_coords/{ysample}_genomecoords.txt",
#        xcoords = "synteny_analysis/genome_coords/x_genome_coords/{xsample}_genomecoords.txt",
#        xprottoloc = "unwrapped_RBH/RBH/reciprocal_best_hits.chrom",
#        yprottoloc = lambda wildcards: config["yaxisspecies"][wildcards.ysample]["prot_to_loc"],
#        recip = "unwrapped_RBH/hmm/searches_agg_best/{ysample}_hmm_best.tsv"
#    output:
#        table = "synteny_analysis/dvalue_table/{xsample}_and_{ysample}_info.tsv",
#        plot_order = "synteny_analysis/plot_order/{xsample}_and_{ysample}_plotorder.tsv"
#    threads:
#        1
#    params:
#        xsample  = lambda wildcards: wildcards.xsample,
#        ysample  = lambda wildcards: wildcards.ysample,
#    run:
#        gen_plotting_df(input.ycoords, input.xcoords,
#                     input.xprottoloc, input.yprottoloc,
#                     params.xsample, params.ysample,
#                     input.recip, output.table,
#                     output.plot_order)
#
#rule generate_breaks_file:
#    """
#    This rule handles generating a file that defines the breaks within individual
#     scaffolds. This process will:
#      - automatically define breaks if the user wants
#      - manually add breaks that are defined in the config file
#      - manually remove breaks defined in the config file.
#
#    Input for the rule:
#      - the plotting df
#      - the genomecoords for both axes
#
#    Output for the rule:
#      - a subsection of the plotting df that defines where the breaks are
#    """
#    input:
#        table = "synteny_analysis/dvalue_table/{xsample}_and_{ysample}_info.tsv",
#        ycoords = "synteny_analysis/genome_coords/y_genome_coords/{ysample}_genomecoords.txt",
#        xcoords = "synteny_analysis/genome_coords/x_genome_coords/{xsample}_genomecoords.txt",
#    output:
#        xtable = "synteny_analysis/dvalue_table_breaks/{xsample}_and_{ysample}_xbreaks_{man_or_auto}.tsv",
#        ytable = "synteny_analysis/dvalue_table_breaks/{xsample}_and_{ysample}_ybreaks_{man_or_auto}.tsv",
#    threads:
#        1
#    params:
#        man_or_auto = lambda wildcards: wildcards.man_or_auto,
#        xsample  = lambda wildcards: wildcards.xsample,
#        ysample  = lambda wildcards: wildcards.ysample,
#        ybreaks  = lambda wildcards: config["yaxisspecies"][wildcards.ysample]["manual_breaks"],
#    run:
#        # the breaks are a dict have scafs as key, list of breaks as the values
#        x_breaks = {}
#        y_breaks = {}
#        for string in params.ybreaks:
#            splitd   = string.split(":")
#            scaf     = splitd[0]
#            position = int(splitd[1])
#            if scaf not in y_breaks:
#                y_breaks[scaf] = set()
#            y_breaks[scaf].add(position)
#        x_offset = genome_coords_to_plotstart_dict(input.xcoords)
#        y_offset = genome_coords_to_plotstart_dict(input.ycoords)
#
#        print("printing the input before finding the breaks")
#        df = pd.read_csv(input.table, delimiter="\t", index_col=0)
#        df["xgene"] = df["xgene"].astype(str)
#        df["ygene"] = df["ygene"].astype(str)
#        print(df)
#
#        if params.man_or_auto == "manual":
#            # parse the manual breaks in the config file
#            print("  - Getting the manual breaks for x")
#            xdf = determine_breaks(df, x_breaks, x_offset, "x", False)
#            print("  - Getting the manual breaks for y")
#            ydf = determine_breaks(df, y_breaks, y_offset, "y", False)
#            # save the results to a file
#            xdf.to_csv(output.xtable, sep="\t")
#            ydf.to_csv(output.ytable, sep="\t")
#        elif params.man_or_auto == "auto":
#            # auto-determine the breaks if we need them
#            print("  - Determining the new xbreaks automatically.")
#            xdf = determine_breaks(df, x_breaks, x_offset, "x", True)
#            print("  - Determining the new ybreaks automatically.")
#            ydf = determine_breaks(df, y_breaks, y_offset, "y", True)
#            # save the results to a file
#            xdf.to_csv(output.xtable, sep="\t")
#            ydf.to_csv(output.ytable, sep="\t")
#
#def synteny_plot(plotting_df,    xcoords_file,  ycoords_file,
#                 xsample,        ysample,
#                 xbreaks_file,   ybreaks_file,
#                 synplot,        prot_to_color,  dropmissing,
#                 plot_x_lines = False,
#                 plot_y_lines = False,
#                 xprottoloc = False,
#                 yprottoloc = False, dropNone = False):
#    """
#    If the user provided a plot order, then we should not skip any scaffolds.
#
#    This is the main plotting script for the synteny plot
#    """
#    import pandas as pd
#    import seaborn as sns; sns.set()
#    import matplotlib
#    import matplotlib.pyplot as plt
#    import matplotlib.ticker as ticker
#    #import matplotlib.patches as mplpatches
#    from matplotlib.ticker import StrMethodFormatter, NullFormatter
#    import numpy as np
#    # set seaborn stuff
#    #sns.set(rc={'text.usetex' : True})
#    sns.set_style("ticks", {'font.family': ['sans-serif'],
#                                'font.sans-serif': ['Helvetica'],
#                                'grid.color': '.95'})
#    # Preserve the vertical order of embedded images:
#    matplotlib.rcParams['image.composite_image'] = False
#    # text as font in pdf
#    matplotlib.rcParams['pdf.fonttype'] = 42
#    matplotlib.rcParams['ps.fonttype'] = 42
#
#    # open the xbreaks and the ybreaks dataframes
#    # These are where there are low-opacity,
#    #  dotted lines to show breaks in synteny
#    # example entries are like this:
#    xbreaks_df = pd.read_csv(xbreaks_file, delimiter="\t", index_col=0)
#    xbreaks_df["xgene"] = xbreaks_df["xgene"].astype(str)
#    xbreaks_df["ygene"] = xbreaks_df["ygene"].astype(str)
#    ybreaks_df = pd.read_csv(ybreaks_file, delimiter="\t", index_col=0)
#    ybreaks_df["xgene"] = ybreaks_df["xgene"].astype(str)
#    ybreaks_df["ygene"] = ybreaks_df["ygene"].astype(str)
#
#
#    # first make a lookup table of how to calculate the
#    #  x and y coords. This lookup is just the amount of
#    # bp to add to the value when plotting. We pass the xprot_to_loc,
#    #  xprot_to_scaf in case we need to sort everything based on order of
#    #  occurrence on the scaffolds
#    x_offset, x_scaf_to_len, vertical_lines_at, xmax, xticklabel, xtickpos, xorder = parse_coords(
#        xcoords_file, xsample, "x", dropNone = dropNone)
#    print("found {} x chromosomes".format(len(x_offset)))
#
#    y_offset, y_scaf_to_len, horizontal_lines_at, ymax, yticklabel, ytickpos, yorder = parse_coords(
#        ycoords_file, ysample, "y")
#    print("found {} y chromosomes".format(len(y_offset)))
#
#    # first make a lookup table
#    df = pd.read_csv(plotting_df, delimiter="\t", index_col=0)
#    df["xgene"] = df["xgene"].astype(str)
#    df["ygene"] = df["ygene"].astype(str)
#
#    xgene = df["xgene"]
#    x = df["xmiddle"]
#    y = df["ymiddle"]
#    bitscore = df["bitscore"]
#
#    print("found {} points to plot".format(len(df["xmiddle"])))
#    print("max bitscore: ", max(bitscore))
#    bitscore_adjusted = [(x/max(bitscore))*60 for x in bitscore]
#    colors = [(0, 0, 1.0, min( 1.0, (x/max(bitscore))*(max(bitscore)/np.mean(bitscore)))) for x in bitscore]
#    drops = set()
#    if prot_to_color:
#        for i in range(len(xgene)):
#            alpha = colors[i][3]
#            try:
#                newcolor = list(matplotlib.colors.to_rgba(prot_to_color[xgene[i]]))
#            except:
#                print("couldn't find a color for: {}".format(xgene[i]))
#                newcolor = [0,0,0,1]
#                drops.add(i)
#            newcolor[3] = alpha
#            colors[i] = newcolor
#
#    if dropmissing:
#        vararray = [xgene, x, y, bitscore, bitscore_adjusted, colors]
#        for j in range(len(vararray)):
#            vararray[j] = [vararray[j][i] for i in range(len(vararray[j])) if i not in drops]
#        xgene = vararray[0]
#        x     = vararray[1]
#        y     = vararray[2]
#        bitscore = vararray[3]
#        bitscore_adjusted = vararray[4]
#        colors = vararray[5]
#
#    # now make a scatter plot
#    figWidth = 8
#    figHeight = 8
#    plt.figure(figsize=(figWidth,figHeight))
#    #set the panel dimensions
#    panelWidth = 4
#    panelHeight = 4
#    dpanel_width = 0.25
#    #find the margins to center the panel in figure
#    leftMargin = (figWidth - panelWidth)/2
#    bottomMargin = ((figHeight - panelHeight)/2)
#    panel1 = plt.axes([leftMargin/figWidth, #left
#                         bottomMargin/figHeight,    #bottom
#                         panelWidth/figWidth,   #width
#                         panelHeight/figHeight])     #height
#    panelxd = plt.axes([leftMargin/figWidth, #left
#                         (bottomMargin+panelHeight+0.1)/figHeight,    #bottom
#                         panelWidth/figWidth,   #width
#                         dpanel_width/figHeight])     #height
#    panelyd = plt.axes([(leftMargin+panelWidth + 0.1)/figWidth, #left
#                         bottomMargin/figHeight,    #bottom
#                         dpanel_width/figWidth,   #width
#                         panelHeight/figHeight])     #height
#    panel1.tick_params(axis='both',which='both',
#                        bottom=False, labelbottom=True,
#                        left=False, labelleft=True,
#                        right=False, labelright=False,
#                        top=False, labeltop=False)
#    panelxd.tick_params(axis='both',which='both',
#                        bottom=False, labelbottom=False,
#                        left=False, labelleft=False,
#                        right=False, labelright=False,
#                        top=False, labeltop=False)
#    panelyd.tick_params(axis='both',which='both',
#                        bottom=False, labelbottom=False,
#                        left=False, labelleft=False,
#                        right=False, labelright=False,
#                        top=False, labeltop=False)
#    # set the panel linewidth thinner
#    for this_panel in [panel1, panelxd, panelyd]:
#        for axis in ['top','bottom','left','right']:
#            this_panel.spines[axis].set_linewidth(0.5)
#    # turn off the axis spines
#    for this_panel in [panelxd, panelyd]:
#        this_panel.spines['top'].set_visible(False)
#        this_panel.spines['right'].set_visible(False)
#
#    panel1.scatter(x, y, color = colors,
#                   ec = None, s=6, linewidths = 0)
#    print("xmax is")
#    print(xmax)
#    # set mins and max
#    panel1.set_xlim([0, xmax])
#    panel1.set_ylim([0, ymax])
#
#    # set x ticks
#    newarr = []
#    newarrlabels=[]
#    if not plot_x_lines:
#        #there are inevitably going to be many scaffolds. We need to subset
#        # get a list of evenly spaced indices
#        numElems = min(20, len(xtickpos)) # this could break if there are fewer elements
#        arr = xtickpos
#        idx = np.round(np.linspace(0, len(arr) - 1, numElems)).astype(int)
#        newarr       = [arr[i] for i in idx]
#        newarrNumScaf     = [i for i in idx]
#        # turn on y-axis ticks on the left - plot scaffolds
#        panel1.tick_params(bottom=True)
#        panel1.set_xticks(newarr)
#        panel1.set_xticklabels(newarrNumScaf, fontsize=8, rotation=90)
#        panel1.set_xlabel(xsample + " number of scaffolds")
#    else:
#        newarr = [0] + list(x_offset.values())
#        panel1.set_xticks(xtickpos)
#        panel1.set_xticklabels(xticklabel, fontsize=8, rotation = 90)
#        panel1.set_xlabel(xsample + " scaffolds")
#    # turn on x-axis ticks on the Dx plot
#    newarrlabels = [round(x/1000000, 1) for x in newarr]
#    panelxd.tick_params(top=True, labeltop=True)
#    panelxd.set_xticks(newarr)
#    panelxd.set_xticklabels(newarrlabels, fontsize=8, rotation=90)
#    panelxd.xaxis.set_label_position("top")
#    panelxd.set_xlabel("Mb")
#
#    # set y ticks
#    newarr=[]
#    newarrlabels=[]
#    if not plot_y_lines:
#        #there are inevitably going to be many scaffolds. We need to subset
#        # get a list of evenly spaced indices
#        numElems = min(20, len(ytickpos))
#        arr = ytickpos
#        idx = np.round(np.linspace(0, len(arr) - 1, numElems)).astype(int)
#        newarr       = [arr[i] for i in idx]
#        newarrlabels = [round(arr[i]/1000000, 1) for i in idx]
#        newarrNumScaf     = [i for i in idx]
#        # turn on y-axis ticks on the left - plot scaffolds
#        panel1.tick_params(left=True)
#        panel1.set_yticks(newarr)
#        panel1.set_yticklabels(newarrNumScaf, fontsize=8)
#        panel1.set_ylabel(ysample + " number of scaffolds")
#    else:
#        newarr = [0] + horizontal_lines_at + [ymax]
#        panel1.set_yticks(ytickpos)
#        panel1.set_yticklabels(yticklabel, fontsize=8)
#        panel1.set_ylabel(ysample + " scaffolds")
#    # turn on y-axis ticks on the Dy plot
#    newarrlabels = [round(x/1000000, 1) for x in newarr]
#    panelyd.tick_params(right=True, labelright=True)
#    panelyd.set_yticks(newarr)
#    panelyd.set_yticklabels(newarrlabels, fontsize=8)
#    panelyd.yaxis.set_label_position("right")
#    panelyd.set_ylabel("Mb")
#
#    # set the x and y labels on Dy and Dx
#    panelxd.bar(x = df["Dx_barmiddle"], height=df["Dx"], width = df["Dx_barwidth"],
#                align = "center", lw=0, color="blue", zorder = 2)
#    panelxd.set_xlim([0,xmax])
#    panelxd.set_ylabel('Dx', fontsize=10)
#
#    panelyd.barh(y = df["Dy_barmiddle"], width=df["Dy"], height = df["Dy_barwidth"],
#                 align = "center", lw=0, color="blue", zorder = 2)
#    panelyd.set_ylim([0,ymax])
#    panelyd.set_xlabel('Dy', fontsize=10)
#
#    for this_axis in [panel1, panelxd, panelyd]:
#        this_axis.xaxis.get_offset_text().set_visible(False)
#        this_axis.yaxis.get_offset_text().set_visible(False)
#
#    #plot vertical lines
#    if plot_x_lines:
#        for value in vertical_lines_at:
#            panel1.axvline(x=value, color="black", lw=0.5)
#    #plot horizontal lines
#    if plot_y_lines:
#        for value in horizontal_lines_at:
#            panel1.axhline(y=value, color="black", lw=0.5)
#
#    # plot vertical BOS
#    for value in xbreaks_df["xmiddle"]:
#        panel1.axvline(x=value, color=[0,0,0,0.25], lw=0.5, linestyle="dotted")
#    # plot horizontal BOS
#    for value in ybreaks_df["ymiddle"]:
#        panel1.axhline(y=value, color=[0,0,0,0.25], lw=0.5, linestyle="dotted")
#    plt.savefig(synplot)
#
#
#"""
#This makes the synteny plot without doing any special coloring of the dots
#"""
#rule plot_synteny:
#    input:
#        dtable = "synteny_analysis/dvalue_table/{xsample}_and_{ysample}_info.tsv",
#        plot_order = "synteny_analysis/plot_order/{xsample}_and_{ysample}_plotorder.tsv",
#        ycoords = "synteny_analysis/genome_coords/y_genome_coords/{ysample}_genomecoords.txt",
#        xcoords = "synteny_analysis/genome_coords/x_genome_coords/{xsample}_genomecoords.txt",
#        xbreaks_auto = "synteny_analysis/dvalue_table_breaks/{xsample}_and_{ysample}_xbreaks_auto.tsv",
#        ybreaks_auto = "synteny_analysis/dvalue_table_breaks/{xsample}_and_{ysample}_ybreaks_auto.tsv",
#        xbreaks_manual = "synteny_analysis/dvalue_table_breaks/{xsample}_and_{ysample}_xbreaks_manual.tsv",
#        ybreaks_manual = "synteny_analysis/dvalue_table_breaks/{xsample}_and_{ysample}_ybreaks_manual.tsv",
#        xprottoloc = "unwrapped_RBH/RBH/reciprocal_best_hits.chrom",
#        yprottoloc = lambda wildcards: config["yaxisspecies"][wildcards.ysample]["prot_to_loc"],
#        recip = "unwrapped_RBH/hmm/searches_agg_best/{ysample}_hmm_best.tsv"
#    output:
#        synplot_manual = "synteny_analysis/plots/synteny_uncolored/{xsample}_and_{ysample}_synteny_manualbreaks.pdf",
#        synplot_noNone = "synteny_analysis/plots/synteny_uncolored/{xsample}_and_{ysample}_synteny_manualbreaks_noNone.pdf",
#    threads:
#        1
#    params:
#        xsample  = lambda wildcards: wildcards.xsample,
#        ysample  = lambda wildcards: wildcards.ysample,
#        keep_x   = True,
#        keep_y   = lambda wildcards: False if ( ("sort_by_x_coord_blast" in config["yaxisspecies"][wildcards.ysample]) or ("noylines" in config["yaxisspecies"][wildcards.ysample])) else True,
#    run:
#        # plot with None ALG
#        synteny_plot(input.dtable,   input.xcoords,  input.ycoords,
#                     params.xsample, params.ysample,
#                     input.xbreaks_manual, input.ybreaks_manual,
#                     output.synplot_manual, None,           False,
#                     plot_x_lines = params.keep_x,
#                     plot_y_lines = params.keep_y,
#                     dropNone = False)
#        # plot without None ALG
#        synteny_plot(input.dtable,   input.xcoords,  input.ycoords,
#                     params.xsample, params.ysample,
#                     input.xbreaks_manual, input.ybreaks_manual,
#                     output.synplot_noNone, None,           False,
#                     plot_x_lines = params.keep_x,
#                     plot_y_lines = params.keep_y,
#                     dropNone = True)
#
#
#def sizes_and_scale_minp(smallest_p):
#    """
#    This outputs two lists. One of scale, and one of sizes,
#    based on the smallest_p.
#
#    This is used for plotting elipses in the figure and the legend
#    """
#    scale     = [1,       0.84,     0.67,     0.5,    0.3   ]
#
#    if smallest_p == 0:
#        sizes = [0,       "1E-20", "1E-10", "1E-5", "1E-2"]
#    elif smallest_p <= float("1E-20"):
#        sizes = ["1E-20", "1E-15", "1E-7",  "1E-4", "5E-2"]
#    elif smallest_p <= float("1E-15"):
#        sizes = ["1E-15", "1E-10", "1E-5",  "1E-2", "1E-1"]
#    elif smallest_p <= float("1E-10"):
#        sizes = ["1E-10", "1E-7",  "1E-4",  "1E-2", "1E-1"]
#    elif smallest_p <= float("1E-8"):
#        sizes = ["1E-8",  "1E-6",  "1E-4",  "1E-2", "1E-1"]
#    elif smallest_p <= float("1E-6"):
#        sizes = ["1E-6",  "1E-5",  "1E-3",  "1E-2", "1E-1"]
#    elif smallest_p <= float("1E-5"):
#        sizes = ["1E-5",  "1E-4",  "1E-3",  "1E-2", "0.5"]
#    else:
#        sizes = ["1E-4",  "1E-3",  "1E-2",  "1E-1", "0.5"]
#    return [scale, sizes]
#
#def quantize_pvalue(xsize, ysize, p, smallest_p):
#    """
#    This quantizes the pvalues using standard values cutoffs
#    returns [xsize, ysize]
#    """
#    scale, sizes = sizes_and_scale_minp(smallest_p)
#    for i in range(len(sizes)):
#        if p <= float(sizes[i]):
#            return [xsize * scale[i], ysize * scale[i]]
#    return [xsize * 0.15, ysize * 0.15]
#
#def fishers_exact(plotting_df,
#                  xcoords_file, ycoords_file,
#                  xsample,      ysample,
#                  xbreaks_file, ybreaks_file,
#                  outdf, style):
#    """
#    This test performs fishers exact test on the genome based on reciprocal
#     best hits, and breaks.
#
#    There are two styles, "whole" & "breaks"
#      - "whole" is conducted by performing Fisher's exact on whole scaffolds
#      - "breaks" is conducted by performing the test on pieces of scaffolds
#    """
#    from scipy import stats
#    # open the xbreaks and the ybreaks dataframes
#    # These are where there are low-opacity,
#    #  dotted lines to show breaks in synteny
#    # example entries are like this:
#    xbreaks_df = pd.read_csv(xbreaks_file, delimiter="\t", index_col=0)
#    ybreaks_df = pd.read_csv(ybreaks_file, delimiter="\t", index_col=0)
#
#    # first make a lookup table of how to calculate the
#    #  x and y coords. This lookup is just the amount of
#    # bp to add to the value when plotting. We pass the xprot_to_loc,
#    #  xprot_to_scaf in case we need to sort everything based on order of
#    #  occurrence on the scaffolds
#    x_offset, x_scaf_to_len, vertical_lines_at, xmax, xticklabel, xtickpos, xorder = parse_coords(
#        xcoords_file, xsample, "x")
#    print("found {} x chromosomes".format(len(x_offset)))
#
#    y_offset, y_scaf_to_len, horizontal_lines_at, ymax, yticklabel, ytickpos, yorder = parse_coords(
#        ycoords_file, ysample, "y")
#
#    # first make a lookup table
#    df = pd.read_csv(plotting_df, delimiter="\t", index_col=0)
#    df["xgene"] = df["xgene"].astype(str)
#    df["ygene"] = df["ygene"].astype(str)
#
#    fisher_dict = []
#    if style == "breaks":
#        xranges_to_dfdict = {}
#        yranges_to_dfdict = {}
#        print("generating breaks dfs for x scaffolds")
#        for xscaf in xorder:
#            plot_xmin  = x_offset[xscaf]
#            plot_xmax  = x_offset[xscaf] + x_scaf_to_len[xscaf]
#            subxdf = xbreaks_df.loc[xbreaks_df["xscaf"] == xscaf, ]
#            subxdf = subxdf.sort_values(by=["xstop"])
#            xbreaks = [plot_xmin] + list(subxdf["xstop"]) + [plot_xmax]
#            for xi in range(len(xbreaks)-1):
#                plotleft   = xbreaks[xi]
#                plotright  = xbreaks[xi+1]
#                query = "xmiddle >= {} & xmiddle < {}".format(plotleft, plotright)
#                inxdf  = df.query(query)
#                outxdf = df[~df.isin(inxdf).all(1)]
#                interval = "{}:{}-{}".format(xscaf, plotleft, plotright)
#                xranges_to_dfdict[interval] = {"in": inxdf, "out": outxdf}
#        print("generating breaks dfs for y scaffolds")
#        for yscaf in yorder:
#            plot_ymin  = y_offset[yscaf]
#            plot_ymax  = y_offset[yscaf] + y_scaf_to_len[yscaf]
#            subydf = ybreaks_df.loc[ybreaks_df["yscaf"] == yscaf, ]
#            subydf = subydf.sort_values(by=["ystop"])
#            ybreaks = [plot_ymin] + list(subydf["ystop"]) + [plot_ymax]
#            for yi in range(len(ybreaks)-1):
#                plotbottom = ybreaks[yi]
#                plottop    = ybreaks[yi+1]
#                query = "ymiddle >= {} & ymiddle < {}".format(plotbottom, plottop)
#                inydf  = df.query(query)
#                outydf = df[~df.isin(inydf).all(1)]
#                interval = "{}:{}-{}".format(yscaf, plotbottom, plottop)
#                yranges_to_dfdict[interval] = {"in": inydf, "out": outydf}
#        # now that we have each interval and their respective dfs, we
#        #  iterate through everything and make the fisher's exact test entry
#        counter = 0
#        num_variations = len(xranges_to_dfdict) * len(yranges_to_dfdict)
#        for xinterval in xranges_to_dfdict:
#            for yinterval in yranges_to_dfdict:
#                counter += 1
#                print("\r    Completed {}/{} interval calculations   ".format(counter, num_variations), end = "")
#                xscaf     = xinterval.split(":")[0]
#                plotleft  = int(float(xinterval.split(":")[1].split("-")[0]))
#                plotright = int(float(xinterval.split(":")[1].split("-")[1]))
#                plotwidth  = plotright - plotleft
#                yscaf     = yinterval.split(":")[0]
#                plotbottom  = int(float(yinterval.split(":")[1].split("-")[0]))
#                plottop     = int(float(yinterval.split(":")[1].split("-")[1]))
#                plotheight = plottop - plotbottom
#                inxdf     = xranges_to_dfdict[xinterval]["in"]
#                outxdf     = xranges_to_dfdict[xinterval]["out"]
#                inydf     = yranges_to_dfdict[yinterval]["in"]
#                outydf     = yranges_to_dfdict[yinterval]["out"]
#                inX_inY   = len(inxdf.merge(inydf))
#                inX_outY  = len(inxdf.merge(outydf))
#                outX_inY  = len(outxdf.merge(inydf))
#                outX_outY = len(outxdf.merge(outydf))
#                table = [[inX_inY, outX_inY], [inX_outY, outX_outY]]
#                oddsratio, pvalue = stats.fisher_exact(table, alternative="greater")
#                fisher_dict.append({"xscaf": xscaf, "yscaf": yscaf,
#                                    "inX_inY": inX_inY, "inX_outY": inX_outY,
#                                    "outX_inY": outX_inY, "outX_outY": outX_outY,
#                                    "oddsratio": oddsratio, "pvalue": pvalue,
#                                    "plotleft": plotleft,
#                                    "plotright": plotright,
#                                    "plotwidth": plotwidth,
#                                    "plotbottom": plotbottom,
#                                    "plottop": plottop,
#                                    "plotheight": plotheight,
#                                    "x_plot_pos": xtickpos[xticklabel.index(xscaf)],
#                                    "y_plot_pos": ytickpos[yticklabel.index(yscaf)]})
#
#    if style == "whole": # perform the analysis on whole chromosomes
#        for xscaf in xorder: #Li
#            for yscaf in yorder: #Wi
#                #calculate the cells for fisher exact
#                plotleft = x_offset[xscaf]
#                plotright = x_offset[xscaf] + x_scaf_to_len[xscaf]
#                plotwidth = plotright - plotleft
#                plotbottom = y_offset[yscaf]
#                plottop    = y_offset[yscaf] + y_scaf_to_len[yscaf]
#                plotheight = plottop-plotbottom
#                query = "yscaf == '{}' & xscaf == '{}'".format(yscaf, xscaf)
#                inX_inY   = len(df.query(query))
#                query = "yscaf == '{}' & xscaf != '{}'".format(yscaf, xscaf)
#                inX_outY  = len(df.query(query))
#                query = "yscaf != '{}' & xscaf == '{}'".format(yscaf, xscaf)
#                outX_inY  = len(df.query(query))
#                query = "yscaf != '{}' & xscaf != '{}'".format(yscaf, xscaf)
#                outX_outY = len(df.query(query))
#                table = [[inX_inY, outX_inY], [inX_outY, outX_outY]]
#                oddsratio, pvalue = stats.fisher_exact(table, alternative="greater")
#                fisher_dict.append({"xscaf": xscaf, "yscaf": yscaf,
#                                    "inX_inY": inX_inY, "inX_outY": inX_outY,
#                                    "outX_inY": outX_inY, "outX_outY": outX_outY,
#                                    "oddsratio": oddsratio, "pvalue": pvalue,
#                                    "plotleft": plotleft,
#                                    "plotright": plotright,
#                                    "plotwidth": plotwidth,
#                                    "plotbottom": plotbottom,
#                                    "plottop": plottop,
#                                    "plotheight": plotheight,
#                                    "x_plot_pos": xtickpos[xticklabel.index(xscaf)],
#                                    "y_plot_pos": ytickpos[yticklabel.index(yscaf)]})
#    fisherdf = pd.DataFrame(fisher_dict)
#    fisherdf.to_csv(outdf, sep="\t")
#
#rule gen_fishertable_whole:
#    """
#    generates a table of fisher's exact test results for whole chromosomes
#    output is a df that can be read back into pandas
#    """
#    input:
#        dtable = "synteny_analysis/dvalue_table/{xsample}_and_{ysample}_info.tsv",
#        ycoords = "synteny_analysis/genome_coords/y_genome_coords/{ysample}_genomecoords.txt",
#        xcoords = "synteny_analysis/genome_coords/x_genome_coords/{xsample}_genomecoords.txt",
#        xbreaks_manual = "synteny_analysis/dvalue_table_breaks/{xsample}_and_{ysample}_xbreaks_manual.tsv",
#        ybreaks_manual = "synteny_analysis/dvalue_table_breaks/{xsample}_and_{ysample}_ybreaks_manual.tsv",
#    output:
#        fisherswhole   = "synteny_analysis/plots/significance/tables/{xsample}_and_{ysample}_fisher_wholechr.tsv"
#    threads:
#        1
#    params:
#        xsample = lambda wildcards: wildcards.xsample,
#        ysample = lambda wildcards: wildcards.ysample,
#        style   = "whole",
#    run:
#        fishers_exact(input.dtable,
#                      input.xcoords,         input.ycoords,
#                      params.xsample,        params.ysample,
#                      input.xbreaks_manual,  input.ybreaks_manual,
#                      output.fisherswhole,   params.style)
#
#rule gen_fishertable_manualbreaks:
#    """
#    generates a table of fisher's exact test results for the manual breaks chromosomes
#    output is a df that can be read back into pandas
#    """
#    input:
#        dtable = "synteny_analysis/dvalue_table/{xsample}_and_{ysample}_info.tsv",
#        ycoords = "synteny_analysis/genome_coords/y_genome_coords/{ysample}_genomecoords.txt",
#        xcoords = "synteny_analysis/genome_coords/x_genome_coords/{xsample}_genomecoords.txt",
#        xbreaks_manual = "synteny_analysis/dvalue_table_breaks/{xsample}_and_{ysample}_xbreaks_manual.tsv",
#        ybreaks_manual = "synteny_analysis/dvalue_table_breaks/{xsample}_and_{ysample}_ybreaks_manual.tsv",
#    output:
#        fishersmanualbreaks   = "synteny_analysis/plots/significance/tables/{xsample}_and_{ysample}_fisher_manualbreaks.tsv"
#    threads:
#        1
#    params:
#        xsample = lambda wildcards: wildcards.xsample,
#        ysample = lambda wildcards: wildcards.ysample,
#        style   = "breaks",
#    run:
#        fishers_exact(input.dtable,
#                      input.xcoords,         input.ycoords,
#                      params.xsample,        params.ysample,
#                      input.xbreaks_manual,  input.ybreaks_manual,
#                      output.fishersmanualbreaks,   params.style)
#
#rule gen_fishertable_autobreaks:
#    """
#    generates a table of fisher's exact test results for the manual breaks chromosomes
#    output is a df that can be read back into pandas
#    """
#    input:
#        dtable = "synteny_analysis/dvalue_table/{xsample}_and_{ysample}_info.tsv",
#        ycoords = "synteny_analysis/genome_coords/y_genome_coords/{ysample}_genomecoords.txt",
#        xcoords = "synteny_analysis/genome_coords/x_genome_coords/{xsample}_genomecoords.txt",
#        xbreaks_auto = "synteny_analysis/dvalue_table_breaks/{xsample}_and_{ysample}_xbreaks_auto.tsv",
#        ybreaks_auto = "synteny_analysis/dvalue_table_breaks/{xsample}_and_{ysample}_ybreaks_auto.tsv",
#    output:
#        fishersautobreaks   = "synteny_analysis/plots/significance/tables/{xsample}_and_{ysample}_fisher_autobreaks.tsv"
#    threads:
#        1
#    params:
#        xsample = lambda wildcards: wildcards.xsample,
#        ysample = lambda wildcards: wildcards.ysample,
#        style   = "breaks",
#    run:
#        fishers_exact(input.dtable,
#                      input.xcoords,         input.ycoords,
#                      params.xsample,        params.ysample,
#                      input.xbreaks_auto,  input.ybreaks_auto,
#                      output.fishersautobreaks,   params.style)
#
#def scale_pvalue_to_0_1(vector, reverse = False):
#    """
#    scales a value to between 0 and 1 given the min and max values seen in that vector
#
#    we might pass the log, so log(0) = inf. Look for that as special case to define minp
#
#    returns a vector
#    """
#    #first find max
#    maxp = -99999999999999
#    minp = 99999999999
#    for entry in vector:
#        if math.isinf(entry) or math.isnan(entry):
#            pass
#        elif entry < minp:
#            minp = entry
#        elif entry > maxp:
#            maxp = entry
#    # now scale everything to between 0 and 1
#    outvec = []
#    for entry in vector:
#        corrected = entry
#        if math.isinf(entry) or math.isnan(entry):
#            corrected = sys.float_info.min
#        thisval = (corrected-minp)/(maxp-minp)
#        if reverse:
#            outvec.append(1-thisval)
#        else:
#            outvec.append(thisval)
#    return outvec
#
