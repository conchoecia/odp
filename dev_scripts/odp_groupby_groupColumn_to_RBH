"""
This script takes the grouby RBH hits file, to which a group column has
  been added. This script unwraps that file into a row-by-row
  reciprocal best hit file.

the input file must be named:
  {sample1}_{sample2}_{sample3}_reciprocal_best_hits.groupby.tsv

... where sample1, sample2, sampleN is the sample name in the config.

There can be as many samples as you'd like.

"""

import ast
from Bio import SeqIO
from Bio import SeqRecord
from Bio import Seq
from itertools import groupby
from itertools import product
import math
import numpy as np
import odp_functions as OdpF
from operator import itemgetter
import pandas as pd
import statistics

configfile: "config.yaml"

OdpF.check_legality(config)

if not "groupby_with_group_column_file" in config:
    raise IOError("You must specify 'groupby_with_group_column_file' in config")

# make sure none of the sample names have underscores
for thissample in config["xaxisspecies"]:
    if "_" in thissample:
        raise IOError("Sample names can't have '_' char: {}".format(thissample))

# come up with all of the analyses
myfile = config["groupby_with_group_column_file"]
species_string = myfile.split("/")[-1].replace("_reciprocal_best_hits.groupby.tsv", "")
all_species = list(sorted(species_string.split("_")))
print("all_species")
print(all_species)

if len(all_species) < 3:
    raise IOError("There must be more than two species.")

analyses_list = [all_species]
print("Here is an example of the first few analyses: {}".format(analyses_list[0]))
print("There are {} possible combinations.".format(len(analyses_list)))

# make sure all of the species in the analyses are in the config
for entry in analyses_list:
    for thisspecies in entry:
        if thisspecies not in config["xaxisspecies"]:
            raise IOError ("You specified {} in the analyses, but it isn't defined in xaxisspecies".format(thisspecies))

config["nways"] = len(all_species)

# come up with number of RBHs
testdf = pd.read_csv(config["groupby_with_group_column_file"], sep = "\t")
maxval = 0
RBH_analyses_list = []
for index, row in testdf.iterrows():
    thislist = ast.literal_eval(row["RBH"])
    for entry in thislist:
        thisnumber = int(entry.split("_")[-1])
        RBH_analyses_list.append(thisnumber)
newstring = "RBH{}way_{}".format(len(all_species), "_".join(sorted(all_species)))
config["RBH_groups"] = ["{}_{}".format(newstring, i) for i in RBH_analyses_list]

# This is specifically for the trio odp
config["yaxisspecies"] = config["xaxisspecies"]
#config["xaxisspecies"]["RBH"] = {
#              "prot_to_loc": "unwrapped_RBH/RBH/reciprocal_best_hits.chrom",
#              "genome": "unwrapped_RBH/RBH/reciprocal_best_hits.fasta"}

#make fake breaks for later analyses
for this_axis in ["yaxisspecies"]:
    if this_axis in config:
        for this_one in config[this_axis]:
            if "manual_breaks" not in config[this_axis][this_one]:
                config[this_axis][this_one]["manual_breaks"] = []
            if "minscafsize" not in config[this_axis][this_one]:
                config[this_axis][this_one]["minscafsize"] = 5000

fisher_files = []
ignore_Fisher = False
if not ignore_Fisher:
    ##make the correlation plots - whole chromosomes
    #outfiles = OdpF.expand_avoid_matching(
    #    "synteny_analysis/plots/significance/wholechr/{xsample}_and_{ysample}_fisher_wholechr.pdf",
    #     xsample = ["RBH"], ysample = config["yaxisspecies"])
    #fisher_files.append(list(outfiles))

    outfiles = OdpF.expand_avoid_matching(
        "synteny_analysis/plots/significance/wholechr_colors/{xsample}_and_{ysample}_fisher_chromcolor.pdf",
         xsample = ["RBH"], ysample = config["yaxisspecies"])
    fisher_files.append(list(outfiles))
    outfiles = OdpF.expand_avoid_matching(
        "synteny_analysis/plots/significance/wholechr_colors/{xsample}_and_{ysample}_fisher_chromcolor_noNone.pdf",
         xsample = ["RBH"], ysample = config["yaxisspecies"])
    fisher_files.append(list(outfiles))
    ## this currently doesn't work - need to trace back where the errors are from
    #outfiles = OdpF.expand_avoid_matching(
    #"synteny_analysis/plots/significance/breaks/{xsample}_and_{ysample}_fisher_manualbreaks.pdf",
    #     xsample = ["RBH"], ysample = config["yaxisspecies"])
    #fisher_files.append(list(outfiles))
    fisher_files = OdpF.flatten(fisher_files)


rule all:
    input:
        #["unwrapped_RBH/RBH/{}_reciprocal_best_hits.tsv".format(
        #    "_".join(thisanalysis)) for thisanalysis in analyses_list],
        # new pseudo genome
        "unwrapped_RBH/RBH/reciprocal_best_hits.chrom",
        "unwrapped_RBH/RBH/reciprocal_best_hits.fasta",
        expand("unwrapped_RBH/fasta/unaligned/{RBH}.fasta",
               RBH = config["RBH_groups"]),
        expand("unwrapped_RBH/hmm/hmms/{RBH}.hmm",
               RBH = config["RBH_groups"]),
        expand("unwrapped_RBH/hmm/searches/{species}/{RBH}_against_{species}.tsv",
               species = all_species, RBH = config["RBH_groups"]),
        expand("unwrapped_RBH/hmm/searches_agg_best/{species}_hmm_best.tsv",
               species = config["yaxisspecies"]),
        # now plot everything
        expand("synteny_analysis/plots/synteny_uncolored/{xsample}_and_{ysample}_synteny_manualbreaks.pdf",
                xsample = ["RBH"], ysample = config["yaxisspecies"]),
        expand("synteny_analysis/plots/synteny_uncolored/{xsample}_and_{ysample}_synteny_manualbreaks_noNone.pdf",
                xsample = ["RBH"], ysample = config["yaxisspecies"]),
        # These need to be rewritten such that x and y do not match
        fisher_files

rule unwrap_RBH_file_with_group_column:
    input:
        RBH_file = config["groupby_with_group_column_file"]
    output:
        RBH_unwrapped = "unwrapped_RBH/RBH/reciprocal_best_hits.tsv"
    threads: 1
    run:
        df = pd.read_csv(input.RBH_file, sep = "\t")

        RBH_entries = []
        for index, row in df.iterrows():
            RBH_list = ast.literal_eval(row["RBH"])
            species_to_gene_list = {}
            species_to_gene_pos = {}
            # make lookup tables
            for thisspecies in all_species:
                # gene_col
                colname = "{}_gene".format(thisspecies)
                species_to_gene_list[thisspecies] = ast.literal_eval(row[colname])
                # pos_col
                colname = "{}_pos".format(thisspecies)
                species_to_gene_pos[thisspecies] = ast.literal_eval(row[colname])

            for i in range(len(RBH_list)):
                thisgroup = row["group"]
                thisentry = {"group": thisgroup,
                             "alpha": row["alpha"],
                             "alpha_type": row["alpha_type"],
                             "RBH": RBH_list[i]}

                for thisspecies in all_species:
                    # get col names
                    scafcol = "{}_scaf".format(thisspecies)
                    genecol = "{}_gene".format(thisspecies)
                    poscol  = "{}_pos".format(thisspecies)
                    thisentry[scafcol] = row[scafcol]
                    thisentry[genecol] = species_to_gene_list[thisspecies][i]
                    thisentry[poscol]  = species_to_gene_pos[thisspecies][i]
                RBH_entries.append(thisentry)

        unwrapped = pd.DataFrame(RBH_entries)
        print(unwrapped)
        print(unwrapped["group"].value_counts())
        unwrapped.to_csv(output.RBH_unwrapped, sep="\t")

rule make_fake_chrom_and_assembly:
    input:
        RBH_unwrapped = "unwrapped_RBH/RBH/reciprocal_best_hits.tsv"
    output:
        RBH_chrom     = "unwrapped_RBH/RBH/reciprocal_best_hits.chrom",
        RBH_fasta     = "unwrapped_RBH/RBH/reciprocal_best_hits.fasta"
    threads: 1
    run:
        df = pd.read_csv(input.RBH_unwrapped, index_col = 0, sep = "\t")
        sort_order = [x for x in sorted(list(df["group"].unique()))
                      if x != "None"] + ["None"]
        chromout = open(output.RBH_chrom, "w")
        genomout = open(output.RBH_fasta, "w")
        for thisscaf in sort_order:
            if (thisscaf != "None") and ("z" not in thisscaf):
                subdf = df.loc[df["group"] == thisscaf,]
                subdf = subdf.sample(frac = 1)

                # sequence
                thisseq = Seq.Seq("".join(["T"] * (len(subdf) * 10000)))
                thisrec = SeqRecord.SeqRecord(thisseq, id = thisscaf)
                SeqIO.write(thisrec, genomout, "fasta")
                # chromfile
                counter = 0
                for index, row in subdf.iterrows():
                    start = (counter * 10000) + 4000
                    stop  = (counter * 10000) + 6000
                    print("{}\t{}\t+\t{}\t{}".format(
                        row["RBH"],
                        row["group"],
                        start, stop), file = chromout)
                    counter += 1
        genomout.close()
        chromout.close()

rule generate_fasta_of_each_group:
    input:
        RBH_unwrapped = "unwrapped_RBH/RBH/reciprocal_best_hits.tsv",
        proteins = lambda wildcards: [config["xaxisspecies"][x]["proteins"]
                    for x in all_species]
    output:
        MBH_fasta = expand("unwrapped_RBH/fasta/unaligned/{RBH}.fasta",
                           RBH = config["RBH_groups"])
    threads: 1
    run:
        df = pd.read_csv(input.RBH_unwrapped, index_col = 0, sep = "\t")

        # make a dict of gene_to_RBH
        species_to_gene_to_RBH = {}
        for index, row in df.iterrows():
            thisRBH = row["RBH"]
            for this_species in all_species:
                if this_species not in species_to_gene_to_RBH:
                    species_to_gene_to_RBH[this_species] = {}
                genecol = "{}_gene".format(this_species)
                species_to_gene_to_RBH[this_species][row[genecol]] = thisRBH
        # now make a dict of RBH_to_fasta_records
        RBH_to_records = {}
        for this_species in all_species:
            inhandle = open(config["xaxisspecies"][this_species]["proteins"], "r")
            for record in SeqIO.parse(inhandle, "fasta"):
                if record.id in species_to_gene_to_RBH[this_species]:
                    thisRBH = species_to_gene_to_RBH[this_species][record.id]
                    record.name = record.id
                    newid = "{}_{}".format(thisRBH, this_species)
                    record.id = newid
                    if thisRBH not in RBH_to_records:
                        RBH_to_records[thisRBH] = []
                    RBH_to_records[thisRBH].append(record)

        # make sure that all entries have three sequences
        for thisRBH in RBH_to_records:
            if len(RBH_to_records[thisRBH]) != len(all_species):
                raise IOError("{} only has {} genes".format(
                    thisRBH, len(RBH_to_records[thisRBH])))
        # print out all of the records to fasta files
        for thisRBH in RBH_to_records:
            print(thisRBH)
            outfile = "unwrapped_RBH/fasta/unaligned/{}.fasta".format(thisRBH)
            with open(outfile, "w") as output_handle:
                for thisrec in RBH_to_records[thisRBH]:
                    SeqIO.write(thisrec, output_handle, "fasta")

rule align_fasta_of_each_group_and_make_hmm:
    input:
        MBH_fasta = "unwrapped_RBH/fasta/unaligned/{RBH}.fasta"
    output:
        aligned = "unwrapped_RBH/fasta/aligned/{RBH}.aligned.fasta",
        hmm     = "unwrapped_RBH/hmm/hmms/{RBH}.hmm"
    threads: 1
    shell:
        """
        mafft --localpair --maxiterate 1000 {input.MBH_fasta} > {output.aligned}
        hmmbuild {output.hmm} {output.aligned}
        """

rule hmm_search_against_genome:
    input:
        proteins = lambda wildcards: config["xaxisspecies"][wildcards.species]["proteins"],
        hmm = "unwrapped_RBH/hmm/hmms/{RBH}.hmm"
    output:
        tsv = "unwrapped_RBH/hmm/searches/{species}/{RBH}_against_{species}.tsv"
    threads: 2
    shell:
        """
        hmmsearch --tblout {output.tsv} \
          --cpu {threads} \
          --noali \
          --notextw \
          {input.hmm} \
          {input.proteins}
        """

rule aggregate_hmm_results_by_species:
    """
    The header fields are:
    # target name        accession  query name                         accession    E-value  score  bias   E-value  score  bias   exp reg clu  ov env dom rep inc description of target

    the headers for blastp outfmt 6 are:
      qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore
    """
    input:
        tsv = expand("unwrapped_RBH/hmm/searches/{{species}}/{RBH}_against_{{species}}.tsv", RBH = config["RBH_groups"])
    output:
        tsv = "unwrapped_RBH/hmm/searches_agg/{species}_hmm_results.tsv"
    params:
        this_species = lambda wildcards: wildcards.species
    threads: 1
    run:
        entries = []
        for thisfile in input.tsv:
            suffix = "_against_{}.tsv".format(params.this_species)
            thisRBH = thisfile.split("/")[-1].replace(suffix, "")
            with open(thisfile, "r") as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith("#"):
                        fields = line.split()
                        thisentry = {"qseqid": fields[2].replace(".aligned", ""),
                                     "sseqid": fields[0],
                                     "pident": 50.0,
                                     "length": 50.0,
                                     "mismatch": 0,
                                     "gapopen": 0,
                                     "qstart": 0,
                                     "qend": 0,
                                     "sstart": 0,
                                     "send": 0,
                                     "evalue": float(fields[4]),
                                     "bitscore": float(fields[5]) }
                        entries.append(thisentry)
        df = pd.DataFrame(entries)
        df.to_csv(output.tsv, sep="\t")

rule best_gene_for_each_hmm:
    """
    finds the best hmm for each gene
    """
    input:
        tsv = "unwrapped_RBH/hmm/searches_agg/{species}_hmm_results.tsv",
        chrom = lambda wildcards: config["xaxisspecies"][wildcards.species]["prot_to_loc"],
        RBH = "unwrapped_RBH/RBH/reciprocal_best_hits.tsv"
    output:
        tsv = "unwrapped_RBH/hmm/searches_agg_best/{species}_hmm_best.tsv",
        RBHinfo = "unwrapped_RBH/hmm/RBH_plus_HMM_agg_best/{species}_hmm_chrom_info.tsv",
        groupby = "unwrapped_RBH/hmm/RBH_plus_HMM_agg_best/{species}_hmm_chrom_info.groupby.tsv"
    params:
        species = lambda wildcards: wildcards.species
    threads: 1
    run:
        species_gene_to_chrom = {}
        species_gene_to_pos = {}
        with open(input.chrom, "r") as f:
            for line in f:
                line = line.strip()
                if line:
                    fields = line.split()
                    species_gene_to_chrom[fields[0]] = fields[1]
                    species_gene_to_pos[fields[0]] = int(fields[3])
        df = pd.read_csv(input.tsv, index_col = 0, sep = "\t")
        df = df.sort_values(["sseqid", "evalue"], ascending=[True, True])
        df = df.drop_duplicates(subset = ["sseqid"])
        df = df.sort_values(["qseqid", "evalue"], ascending=[True, True])
        df = df.drop_duplicates(subset = ["qseqid"])
        df = df.reset_index(drop=True)
        df.to_csv(output.tsv, header = False, index = False, sep="\t")
        df = df.iloc[:, 0:2]
        df.columns = ["RBH", params.species]
        RBH = pd.read_csv(input.RBH, index_col = 0, sep = "\t")
        RBH["{}_gene".format(params.species)] = RBH["RBH"].map(
                    dict(zip(df["RBH"],
                             df[params.species])) )
        RBH["{}_scaf".format(params.species)] = RBH[
            "{}_gene".format(params.species)].map(
            species_gene_to_chrom)
        RBH["{}_pos".format(params.species)] = RBH[
            "{}_gene".format(params.species)].map(
            species_gene_to_pos)
        RBH.to_csv(output.RBHinfo,  sep="\t")

        groupbycols = [x for x in RBH.columns if "_scaf" in x]
        grouped_multiple = RBH.groupby(groupbycols).agg(list).reset_index()
        # get the size
        grouped_multiple["count"] = grouped_multiple.RBH.str.len()
        grouped_multiple.to_csv(output.groupby,  sep="\t")

rule get_genome_coords_x:
    input:
        genome = "unwrapped_RBH/RBH/reciprocal_best_hits.fasta"
    output:
        coords = "synteny_analysis/genome_coords/x_genome_coords/{xsample}_genomecoords.txt"
    threads:
        1
    params:
        minsize = 0
    shell:
        """
        bioawk -cfastx '{{ if (length($seq) >= {params.minsize}) {{ \
                           print($name, length($seq), sum)  }} \
                        }}' {input.genome} | \
          awk '{{sum = sum + $2; print($1, $2, sum, sum - $2) }}' > {output.coords}
        """

rule get_genome_coords_y:
    input:
        genome = lambda wildcards: config["yaxisspecies"][wildcards.ysample]["genome"]
    output:
        coords = "synteny_analysis/genome_coords/y_genome_coords/{ysample}_genomecoords.txt",
    threads:
        1
    params:
        minsize = lambda wildcards: config["yaxisspecies"][wildcards.ysample]["minscafsize"]
    shell:
        """
        bioawk -cfastx '{{ if (length($seq) >= {params.minsize}) {{ \
                           print($name, length($seq), sum)  }} \
                        }}' {input.genome} | \
          sort -k2 -nr | \
          awk '{{sum = sum + $2; print($1, $2, sum, sum - $2) }}' > {output.coords}
        """
def genome_coords_to_plotstart_dict(path_to_genocoords_file):
    """
    Takes a genome coords file where:
      - col1: scaf name
      - col2: scaflen
      - col3: cumsum of the total plot size
      - col4: the plot starting position for that scaffold

    sca1 3822568 3822568  0
    sca2 2667796 6490364  3822568
    sca3 2526311 9016675  2667796
    sca4 2410750 11427425 2526311
    sca5 2150379 13577804 2410750
    sca6 1771964 15349768 2150379

    And returns a dict where col1 (scaf name) is key
     and col4 (plotting offset) is the value
    """
    offset_dict = {}
    with open(path_to_genocoords_file, "r") as f:
        for line in f:
            line = line.strip()
            if line:
                splitd = line.split()
                offset_dict[splitd[0]] = int(splitd[3])
    return offset_dict

def genome_coords_to_offset_dict(path_to_genocoords_file):
    """
    Takes a genome coords file where:
      - col1: scaf name
      - col2: scaflen
      - col3: cumsum of the total plot size
      - col4: the plot starting position for that scaffold

    sca1 3822568 3822568  0
    sca2 2667796 6490364  3822568
    sca3 2526311 9016675  2667796
    sca4 2410750 11427425 2526311
    sca5 2150379 13577804 2410750
    sca6 1771964 15349768 2150379

    And returns a dict where col1 (scaf name) is key
     and col3 (plotting offset) is the value
    """
    offset_dict = {}
    with open(path_to_genocoords_file, "r") as f:
        for line in f:
            line = line.strip()
            if line:
                splitd = line.split()
                offset_dict[splitd[0]] = int(splitd[2])
    return offset_dict

def generate_coord_structs_from_chrom_to_loc(prot_to_loc_file):
    """
    This parses a .chrom file and outputs five data structures that are easily
     used for mapping pandas dataframes.
    The output is a dict of dicts. Not the most intuitive format but easy for
     mapping to column values.
     { "prot_to_scaf":   prot_to_scaf,
       "prot_to_strand": prot_to_strand,
       "prot_to_start":  prot_to_start,
       "prot_to_stop":   prot_to_stop,
       "prot_to_middle": prot_to_middle }
    """
    prot_to_scaf   = {}
    prot_to_strand = {}
    prot_to_start  = {}
    prot_to_stop   = {}
    prot_to_middle = {}
    print("prot_to_loc_file", prot_to_loc_file)
    with open(prot_to_loc_file, "r") as f:
       for line in f:
           line = line.strip()
           if line:
               splitd = line.split()
               prot = splitd[0]
               # add things now
               prot_to_scaf[prot]   = splitd[1]
               prot_to_strand[prot] = splitd[2]
               start = int(splitd[3])
               prot_to_start[prot]  = start
               stop = int(splitd[4])
               prot_to_stop[prot]   = stop
               stop = int(splitd[4])
               prot_to_middle[prot] = int(start + (stop - start)/2)
    return { "prot_to_scaf":   prot_to_scaf,
             "prot_to_strand": prot_to_strand,
             "prot_to_start":  prot_to_start,
             "prot_to_stop":   prot_to_stop,
             "prot_to_middle": prot_to_middle }

def blast_plot_order_helper(coords, sample, xory, xprottoloc, yprottoloc, recip,
                            xorder):
    """
    This uses the reciprocal blast results to come up with the sort order
     for the y-axis scaffolds. Returns a list of the plot order.

    This code is all duplicated from the synteny plot function.
     Could be programmed in a better way to avoid redundancy, but this just fits
     the edge case where the y-axis has to be arranged based on the blast results.
    """
    # now make a lookup table of where the prots are.
    #  Use the x_offset and y_offset to recalculate where the plotting
    #  value is
    xcoords = generate_coord_structs_from_chrom_to_loc(xprottoloc)
    ycoords = generate_coord_structs_from_chrom_to_loc(yprottoloc)

    # now open the blast results and translate the pairs
    #  into plotting positions
    df = pd.read_csv(recip, header=None, sep = "\t")
    df.columns = ["xgene", "ygene", "pident", "length",
                  "mismatch", "gapopen", "qstart", "qend",
                  "sstart", "send", "evalue", "bitscore"]
    df = df[["xgene", "ygene", "bitscore", "evalue"]]

    #print(x_prot_to_loc)
    df["xpos"] = df["xgene"].map(xcoords["prot_to_middle"])
    df["ypos"] = df["ygene"].map(ycoords["prot_to_middle"])

    df["xscaf"] = df["xgene"].map(xcoords["prot_to_scaf"])
    df["yscaf"] = df["ygene"].map(xcoords["prot_to_scaf"])
    df = df.dropna()
    df = df.sort_values(by=['xpos'])
    df = df.dropna()

    grouped_df = df.groupby(["yscaf"])
    for key, item in grouped_df:
        max_item = grouped_df.get_group(key)['xscaf'].value_counts().idxmax()
        all_other_things = [x for x in grouped_df.get_group(key)['xscaf'].unique() if x != max_item]
        for thisthing in all_other_things:
            df = df.loc[~( (df["yscaf"] == key) & (df["xscaf"] == thisthing)), ]
    # now sort based on the xscafs and the xpos
    sorterIndex = dict(zip(xorder, range(len(xorder))))
    df.sort_values(['yscaf', 'ypos'],
        ascending = [True, True], inplace = True)
    df.reset_index(drop=True, inplace = True)
    df = df.drop_duplicates(subset=['yscaf'])
    df['x_Rank'] = df['xscaf'].map(sorterIndex)
    df.sort_values(['x_Rank', 'xpos'],
        ascending = [True, True], inplace = True)
    df = df.dropna()
    df.reset_index(drop=True, inplace = True)
    #print(list(df.yscaf))
    return(list(df.yscaf))

def parse_coords(coords_file, sample, xory,
                 xprottoloc=None, yprottoloc=None,
                 recip=None, xorder=None, dropNone = False):
    """
    This parses the coordinates and returns a
      - coord-to-offset dict (I don't remember what this is for),
      - the size of each scaffold (a dictionary)
      - a list of locations to plot lines (These are the scaf/chrom divisions)
      - the max value for that axis
      - the tick labels
      - the tick positions
      - the yorder or xorder
    """
    offset = {}
    max_coord = 0
    lines_at = []
    df = pd.read_csv(coords_file, header = None, sep = " ")
    df.columns = ["scaf", "scaflen", "cumsum", "coordstart"]
    # now figure out if we need to sort or not
    drop_nas = True
    if dropNone:
        df = df.loc[df["scaf"] != "None", ]
    # make sure that plotorder and sort_by_x_coord_blast aren't there together
    #  These are two conflicting sort order operations.
    #  Specifically, sort_by_x_coord_blast will mess up plotorder.

    if xory == "x":
        plotorder = None
    elif xory == "y":
        #print("we're in y")
        if sample not in config["yaxisspecies"]:
            raise IOError("Can't find this yspecies")
        else:
            #print("we're in the else of y")
            if "plotorder" in config["yaxisspecies"][sample]:
                #print("we're in the plotorder of y")
                plotorder = config["yaxisspecies"][sample]["plotorder"]
                #print("plotorder zeroth " + plotorder[0])
                drop_nas = False
            elif "sort_by_x_coord_blast" in config["yaxisspecies"][sample]:
                #print("we're in the sort_by_x_coord_blast of y")
                if config["yaxisspecies"][sample]["sort_by_x_coord_blast"]:
                    #print("we're in the sort_by_x_coord_blast of y True")
                    # we need to set up the sort order based on the occurrence in the blast results
                    plotorder = blast_plot_order_helper(coords_file, sample, xory,
                                                        xprottoloc, yprottoloc,
                                                        recip, xorder)
                    #print("after plot order in sort_by_x_coord_blast")
            else:
                #print("in the else")
                plotorder = None
    else:
        raise IOError("Don't know what this is")
    #print("df after xory")
    # now we have determined if we need to sort
    if plotorder != None: # if plotorder has something in it.
        #print(" - using custom plot order: ", plotorder)
        sortdict = {key: val for key, val in zip(plotorder, range(len(plotorder)))}
        df['rank'] = df['scaf'].map(sortdict)
        df.sort_values(by = 'rank' ,inplace=True)
    #print("df after plotorder")
    #print(df)

    # now, if we made plotorder from config then drop rows
    if xory == "y":
        if "plotorder" in config["{}axisspecies".format(xory)][sample]:
            df = df[df["scaf"].isin(config["{}axisspecies".format(xory)][sample]["plotorder"])]
    # only drop if we haven't specified the plot order in the config
    #if drop_nas:
    #    df = df.dropna()
    df.reset_index(drop=True, inplace = True)
    df["cumsum"] = df["scaflen"].cumsum()
    df["cumsum"] = df["cumsum"] - df["scaflen"]
    print("df after cumulative sum and sorting")
    print(df)
    for i, row in df.iterrows():
        offset[row["scaf"]] = row["cumsum"]
        if i > 0:
            lines_at.append(row["cumsum"])
    
    max_coord = list(df["scaflen"].cumsum())[-1]

    #tick labels
    tick_labels = list(df["scaf"])
    tick_pos    = list(df["cumsum"] + (df["scaflen"]/2))

    scaf_to_len = {}
    for i, row in df.iterrows():
        scaf_to_len[row["scaf"]] = row["scaflen"]

    return (offset, scaf_to_len, lines_at, max_coord, tick_labels, tick_pos, list(df["scaf"]))


def calc_D_for_y_and_x(df, x_offset, y_offset, x_scaf_to_len, y_scaf_to_len):
    """
    This calculates D for both the x and y axes.
    Defined in the 2020 vertebrate synteny paper.
    """
    df = df.dropna()
    # some variable names in this for loop are "x" but it doesn't matter.
    #  everything important is variable between x and y
    for thisdir in ["x", "y"]:
        df = df.sort_values(by=["{}middle".format(thisdir)])
        df.reset_index(drop=True, inplace = True)

        unique_x = df["{}scaf".format(thisdir)].unique()
        thisdir_dfs = []
        # this just calculates Dx
        for thisx in unique_x:
            xdf = df.loc[df["{}scaf".format(thisdir)] == thisx, ].copy()
            xdf = xdf.reset_index(drop=True)
            oppositexy = "d"
            this_offset = {}
            this_scaf_to_len = {}
            if thisdir == "x":
                oppositexy = "y"
                this_offset = x_offset
                this_scaf_to_len = x_scaf_to_len
            elif thisdir == "y":
                oppositexy = "x"
                this_offset = y_offset
                this_scaf_to_len = y_scaf_to_len
            df2 = pd.get_dummies(xdf["{}scaf".format(oppositexy)])
            df2_xiL = df2.apply(lambda x: x.rolling(20).mean(), axis = 0)
            df2_xiR = df2.apply(lambda x: x.iloc[::-1].rolling(20).mean(), axis = 0).iloc[::-1]
            df2_xiR = df2_xiR.set_index(df2_xiR.index - 1)
            df2_xiR = df2_xiR.iloc[1:]
            subtractdf = df2_xiR.fillna(0) - df2_xiL.fillna(0)
            D = subtractdf.apply(lambda x: np.sqrt(np.square(x).sum()), axis = 1)
            xdf["D{}".format(thisdir)] = D
            xdf["D{}_barleft".format(thisdir)] = 0
            xdf["D{}_barmiddle".format(thisdir)] = 0
            xdf["D{}_barright".format(thisdir)] = 0
            xdf["D{}_barwidth".format(thisdir)] = 0
            for i, row in xdf.iterrows():
                barleft   = -1
                barright  = -1
                barmiddle = -1
                barwidth  = -1
                if len(xdf) > 1:
                    if i == 0:
                        thisend   = row["{}stop".format(thisdir)]
                        nextstart = xdf.loc[i+1, "{}start".format(thisdir)]
                        barleft   = this_offset[thisx]
                        barright  = thisend + ((nextstart-thisend)/2)
                    elif i == (len(xdf) - 1):
                        prevend   = xdf.loc[i-1, "{}stop".format(thisdir)]
                        thisstart = row["{}start".format(thisdir)]
                        barleft   = prevend + ((thisstart-prevend)/2)
                        barright  = this_scaf_to_len[thisx]
                    else:
                        prevend   = xdf.loc[i-1, "{}stop".format(thisdir)]
                        thisstart = row["{}start".format(thisdir)]
                        thisend   = row["{}stop".format(thisdir)]
                        nextstart = xdf.loc[i+1, "{}start".format(thisdir)]
                        barleft   = prevend + ((thisstart-prevend)/2)
                        barright  = thisend + ((nextstart-thisend)/2)
                xdf.loc[i, "D{}_barleft".format(thisdir)]   = barleft
                xdf.loc[i, "D{}_barright".format(thisdir)]  = barright
                xdf.loc[i, "D{}_barmiddle".format(thisdir)] = barleft + ((barright - barleft)/2)
                xdf.loc[i, "D{}_barwidth".format(thisdir)]  = barright - barleft + 1
            thisdir_dfs.append(xdf)
        df = pd.concat(thisdir_dfs)
    df = df.sort_values(by=["xmiddle"])
    df.reset_index(drop=True, inplace = True)
    return df

def determine_breaks(df, scaf_to_breaks_set, scaf_to_offset_dict,
                     sort_direction, auto_breaks):
    """
    determines the major breaks in Dx or Dy to use as partitions.

    The input parameters are:
      - df: the analysis df at the end of synteny plot.
      - sort_direction: either "x" or "y"

    The output of this method is a dataframe that is just the rows of the input
     that are the breakpoints in the input df.
    """
    # MAGIC NUMBERS
    # set window to change how many genes on either side are considered when
    #  looking for peaks. A value of 20 means 20 on either side, so 41 genes total
    window = 20
    smallwindow = 5
    # set small_window to resolve nearby peaks from different datasources

    sort_order = {"x": {"pos": "xmiddle",
                        "end": "xstop",
                        "chrom": "xscaf",
                        "D": "Dx"},
                  "y": {"pos": "ymiddle",
                        "end": "ystop",
                        "chrom": "yscaf",
                        "D": "Dy"}}

    # sort the dataframe based on which axis we're looking at
    df = df.sort_values(by=[sort_order[sort_direction]["pos"]])
    df = df.reset_index(drop=True)

    # first, figure out the manual break positions in terms of the protein coordinates
    manual_breaks_indices = set()
    for thisscaf in scaf_to_breaks_set:
        for thisposition in scaf_to_breaks_set[thisscaf]:
            offset_position = thisposition + scaf_to_offset_dict[thisscaf]
            subdf = df.loc[df[sort_order[sort_direction]["end"]] <= offset_position, ]
            subdf = subdf.sort_values(by=[sort_order[sort_direction]["end"]])
            tempdf = df.loc[df[sort_order[sort_direction]["chrom"]] == thisscaf, ["xgene", "ygene", "xstart"]]
            #print(thisscaf, thisposition, offset_position)
            #print(tempdf)
            #print(subdf.loc[subdf.index[-1]])
            #print()
            #sys.exit()
            manual_breaks_indices.add(subdf.index[-1])
    manual_breaks_indices = list(manual_breaks_indices)
    #if not auto_breaks:
    #    print("manual_breaks")
    #    print(manual_breaks_indices)

    all_ranges = set()
    if auto_breaks:
        unique_chroms = []
        #chrom_breakpoints = []
        for index, row in df.iterrows():
            thischrom = row[sort_order[sort_direction]["chrom"]]
            thispos  = row[sort_order[sort_direction]["pos"]]
            if thischrom not in unique_chroms:
                unique_chroms.append(thischrom)
                #chrom_breakpoints.append(thispos)
        ## this line is solely for plotting. Not useful
        #chrom_breakpoints = chrom_breakpoints[1::]

        # There are three different analysis types that we will use to figure
        #  out the seps.
        # - deltMA is the derivative of the smoothed data
        # - deltD is the derivative of the raw data
        # - Dx2 is the raw D data.
        #
        # All of the data are selected based on the max value of what was above
        #  the median for that chromosome.
        for thiscol in ["MA", "deltMA","deltD", "Dx2"]:
            for thischrom in unique_chroms:
                # use .copy() to make sure we're not modifying the original df
                subdf = df.loc[df[sort_order[sort_direction]["chrom"]] == thischrom, ].copy()
                # Dx2 is just the raw data that is above the median
                subdf["Dx2"] = subdf[sort_order[sort_direction]["D"]]
                subdf['Dx2'] = np.where((subdf[sort_order[sort_direction]["D"]] < subdf[sort_order[sort_direction]["D"]].median()),np.NaN,subdf["Dx2"])
                # MA is the moving average of the raw data
                subdf["MA"] = subdf["Dx2"].rolling(window=3, center=True).mean()
                subdf["MA2"] = subdf["Dx2"].rolling(window=19, center=True).mean()
                # deltMA is the derivative of the moving average
                subdf["deltMA"] = subdf["MA"].diff() / subdf["MA"].index.to_series().diff()
                subdf['deltMA'] = np.where((subdf["MA"] < subdf["MA"].median()),np.NaN,subdf["deltMA"])
                # deltD is the derivative of the raw data
                subdf["deltD"] = subdf["Dx2"].diff() / subdf["Dx2"].index.to_series().diff()
                subdf['deltD'] = np.where((subdf.Dx2 < subdf["Dx2"].median()),np.NaN,subdf.deltD)

                # get the groups of consecutive values in each category
                idxmaxes = set()
                ind = list(subdf[~subdf[thiscol].isnull()].index)
                ranges =[]
                for k,g in groupby(enumerate(ind),lambda x:x[0]-x[1]):
                    group = (map(itemgetter(1),g))
                    group = list(map(int,group))
                    ranges.append((group[0],group[-1]))

                # now get the peak from each contiguous range of values
                if len(ranges) > 0:
                    for this_range in ranges:
                        if this_range[0] != this_range[-1]:
                            #this_range = [x for x in range(this_range[0], this_range[1]+1)]
                            this_range = list(this_range)
                            which_d_col = sort_order[sort_direction]["D"]
                            temp = subdf.loc[this_range[0]:this_range[-1]][which_d_col].idxmax()
                            idxmaxes.add(temp)

                # picks the best in large windows of genes.
                #  See the description for the 'window' variable above
                keep_idx_maxes = set()
                ignore_set = set()
                done = False
                consider_ranges = set()
                while not done:
                    consider_ranges = set()
                    # get peaks within the window if they're not in the ignore set
                    for this_idx in idxmaxes:
                        thistup = tuple([x for x in idxmaxes
                             if ((x > this_idx - window)
                                 and (x < this_idx + window)
                                 and (x not in ignore_set))])
                        if len(thistup) > 0:
                            consider_ranges.add(thistup)

                    # now for each set of peaks, get the best in each window
                    consider_ranges = sorted(list(consider_ranges), key=len, reverse=True)
                    if len(consider_ranges) > 0: # skip the empty ranges
                        thisrange = list(consider_ranges[0])
                        if len(thisrange) == 1:
                            done = True
                        else:
                            submax = df.loc[thisrange, ][sort_order[sort_direction]["D"]].idxmax()
                            for thisid in thisrange:
                                if thisid != submax:
                                    ignore_set.add(thisid)
                    else: # if it is empty, leave
                        done = True
                # We found the biggest peaks in the windows, add them to all_ranges
                for entry in consider_ranges:
                    all_ranges.add(entry)

    # flatten the results of what we got from the last analysis
    idxmaxes = OdpF.flatten(all_ranges)
    idxmaxes = OdpF.flatten([idxmaxes, manual_breaks_indices])

    # From the dataset of all peaks, find the best in small windows.
    #  See the variable `smallwindow` above
    # The reason we have this block is that the same peak, or something near it,
    #  could have been added multiple times, at slightly different indices.
    #  This collapses the similar indices to get the best.
    ignore_set = set()
    done = False
    consider_ranges = set()
    while not done:
        consider_ranges = set()
        for this_idx in idxmaxes:
            # get windows of ranges if they're not in the ignore set
            thistup = tuple([x for x in idxmaxes
                 if ((x > this_idx - smallwindow)
                     and (x < this_idx + smallwindow)
                     and (x not in ignore_set))])
            if len(thistup) > 0:
                consider_ranges.add(thistup)

        consider_ranges = sorted(list(consider_ranges), key=len, reverse=True)
        if len(consider_ranges) > 0:
            thisrange = list(consider_ranges[0])
            if len(thisrange) == 1:
                done = True
            else:
                submax = df.loc[thisrange, ][sort_order[sort_direction]["D"]].idxmax()
                for thisid in thisrange:
                    if thisid != submax:
                        ignore_set.add(thisid)
        else:
            # there's nothing here
            done = True

    # vert_lines is the list of indices from the df that have the peaks that
    #  we want to keep.
    vert_lines = OdpF.flatten(consider_ranges)
    # return a dataframe of the intersections we want
    return df.loc[vert_lines].copy()

def gen_plotting_df(ycoords_file, xcoords_file,
                    xprottoloc, yprottoloc,
                    xsample, ysample,
                    recip, outtable, plotorder_file):
    """
    Generates a dataframe that will be used by the other parts of the program
     for plotting.
    Saves it to a file
    """
    import pandas as pd
    import numpy as np
    # set seaborn stuff

    # first make a lookup table of how to calculate the
    #  x and y coords_file. This lookup is just the amount of
    # bp to add to the value when plotting. We pass the xprot_to_loc,
    #  xprot_to_scaf in case we need to sort everything based on order of
    #  occurrence on the scaffolds
    x_offset, x_scaf_to_len, vertical_lines_at, xmax, xticklabel, xtickpos, xorder = parse_coords(
        xcoords_file, xsample, "x")
    print("found {} x chromosomes".format(len(x_offset)))

    y_offset, y_scaf_to_len, horizontal_lines_at, ymax, yticklabel, ytickpos, yorder = parse_coords(
        ycoords_file, ysample, "y",
        xprottoloc, yprottoloc, recip, xticklabel)
    print("found {} y chromosomes".format(len(y_offset)))

    # now save the plot order to a file
    with open(plotorder_file, "w") as f:
        print("xplotorder:", file=f)
        for entry in xorder:
            print("  - {}".format(entry), file=f)
        print("yplotorder:", file=f)
        for entry in yorder:
            print("  - {}".format(entry), file=f)

    # now make a lookup table of where the prots are.
    #  Use the x_offset and y_offset to recalculate where the plotting
    #  value is
    xstruct = generate_coord_structs_from_chrom_to_loc(xprottoloc)
    xstruct["prot_plot_start"] = {}
    xstruct["prot_plot_middle"] = {}
    xstruct["prot_plot_stop"] = {}
    ystruct = generate_coord_structs_from_chrom_to_loc(yprottoloc)
    ystruct["prot_plot_start"] = {}
    ystruct["prot_plot_middle"] = {}
    ystruct["prot_plot_stop"] = {}
    #print("xstruct")
    #print(xstruct)
    # get rid of proteins that we don't need along the x axis
    #  also set the plotting position based on the offset
    for prot in list(xstruct["prot_to_middle"].keys()):
        scaf = xstruct["prot_to_scaf"][prot]
        if scaf not in x_offset:
            for thisdict in xstruct:
                xstruct[thisdict].pop(prot, None)
        else:
            xstruct["prot_plot_start"][prot]  = xstruct["prot_to_start"][prot] + x_offset[scaf]
            xstruct["prot_plot_middle"][prot] = xstruct["prot_to_middle"][prot] + x_offset[scaf]
            xstruct["prot_plot_stop"][prot]   = xstruct["prot_to_stop"][prot] + x_offset[scaf]
    # get rid of proteins that we don't need along the y axis
    #  also set the plotting position based on the offset
    for prot in list(ystruct["prot_to_middle"].keys()):
        scaf = ystruct["prot_to_scaf"][prot]
        if scaf not in y_offset:
            for thisdict in xstruct:
                ystruct[thisdict].pop(prot, None)
        else:
            ystruct["prot_plot_start"][prot]  = ystruct["prot_to_start"][prot] + y_offset[scaf]
            ystruct["prot_plot_middle"][prot] = ystruct["prot_to_middle"][prot] + y_offset[scaf]
            ystruct["prot_plot_stop"][prot]   = ystruct["prot_to_stop"][prot] + y_offset[scaf]

    # now open the blast results and translate the pairs
    #  into plotting positions
    df = pd.read_csv(recip, header=None, sep = "\t")
    df.columns = ["xgene", "ygene", "pident", "length",
                  "mismatch", "gapopen", "qstart", "qend",
                  "sstart", "send", "evalue", "bitscore"]
    df = df[["xgene", "ygene", "bitscore", "evalue"]]
    df["xgene"] = df["xgene"].astype(str)
    df["ygene"] = df["ygene"].astype(str)
    #print(x_prot_to_loc)
    df["xstart"]  = df["xgene"].map(xstruct["prot_plot_start"])
    df["xmiddle"] = df["xgene"].map(xstruct["prot_plot_middle"])
    df["xstop"]   = df["xgene"].map(xstruct["prot_plot_stop"])

    df["ystart"]  = df["ygene"].map(ystruct["prot_plot_start"])
    df["ymiddle"] = df["ygene"].map(ystruct["prot_plot_middle"])
    df["ystop"]   = df["ygene"].map(ystruct["prot_plot_stop"])
    print(df)

    # I don't remember why the x and y is switched here.
    df["yscaf"] = df["ygene"].map(ystruct["prot_to_scaf"])
    df["xscaf"] = df["xgene"].map(xstruct["prot_to_scaf"])
    #df = df.dropna() # this messed up plotting order if there were no hits on that scaf.
    df = df.sort_values(by=['xmiddle'])
    #df = df.loc[df["evalue"] <= float("1E-20"), ]
    df.reset_index(drop=True, inplace = True)

    # Now calculate D and Dw for both X and Y axes
    for thiscol in ['Dx', 'Dx_barleft',
       'Dx_barmiddle', 'Dx_barright', 'Dx_barwidth', 'Dy', 'Dy_barleft',
       'Dy_barmiddle', 'Dy_barright', 'Dy_barwidth']:
        df[thiscol] = 0
    if outtable:
        df.to_csv(outtable, sep="\t")

rule generate_plotting_df_and_plot_order:
    """
    This parses all of the various types of input and generates the df that
     contains the plotting information like genes, reciprocal-best orthologs.

    Also outputs a file of the plotting order.
    """
    input:
        ycoords = "synteny_analysis/genome_coords/y_genome_coords/{ysample}_genomecoords.txt",
        xcoords = "synteny_analysis/genome_coords/x_genome_coords/{xsample}_genomecoords.txt",
        xprottoloc = "unwrapped_RBH/RBH/reciprocal_best_hits.chrom",
        yprottoloc = lambda wildcards: config["yaxisspecies"][wildcards.ysample]["prot_to_loc"],
        recip = "unwrapped_RBH/hmm/searches_agg_best/{ysample}_hmm_best.tsv"
    output:
        table = "synteny_analysis/dvalue_table/{xsample}_and_{ysample}_info.tsv",
        plot_order = "synteny_analysis/plot_order/{xsample}_and_{ysample}_plotorder.tsv"
    threads:
        1
    params:
        xsample  = lambda wildcards: wildcards.xsample,
        ysample  = lambda wildcards: wildcards.ysample,
    run:
        gen_plotting_df(input.ycoords, input.xcoords,
                     input.xprottoloc, input.yprottoloc,
                     params.xsample, params.ysample,
                     input.recip, output.table,
                     output.plot_order)

rule generate_breaks_file:
    """
    This rule handles generating a file that defines the breaks within individual
     scaffolds. This process will:
      - automatically define breaks if the user wants
      - manually add breaks that are defined in the config file
      - manually remove breaks defined in the config file.

    Input for the rule:
      - the plotting df
      - the genomecoords for both axes

    Output for the rule:
      - a subsection of the plotting df that defines where the breaks are
    """
    input:
        table = "synteny_analysis/dvalue_table/{xsample}_and_{ysample}_info.tsv",
        ycoords = "synteny_analysis/genome_coords/y_genome_coords/{ysample}_genomecoords.txt",
        xcoords = "synteny_analysis/genome_coords/x_genome_coords/{xsample}_genomecoords.txt",
    output:
        xtable = "synteny_analysis/dvalue_table_breaks/{xsample}_and_{ysample}_xbreaks_{man_or_auto}.tsv",
        ytable = "synteny_analysis/dvalue_table_breaks/{xsample}_and_{ysample}_ybreaks_{man_or_auto}.tsv",
    threads:
        1
    params:
        man_or_auto = lambda wildcards: wildcards.man_or_auto,
        xsample  = lambda wildcards: wildcards.xsample,
        ysample  = lambda wildcards: wildcards.ysample,
        ybreaks  = lambda wildcards: config["yaxisspecies"][wildcards.ysample]["manual_breaks"],
    run:
        # the breaks are a dict have scafs as key, list of breaks as the values
        x_breaks = {}
        y_breaks = {}
        for string in params.ybreaks:
            splitd   = string.split(":")
            scaf     = splitd[0]
            position = int(splitd[1])
            if scaf not in y_breaks:
                y_breaks[scaf] = set()
            y_breaks[scaf].add(position)
        x_offset = genome_coords_to_plotstart_dict(input.xcoords)
        y_offset = genome_coords_to_plotstart_dict(input.ycoords)

        print("printing the input before finding the breaks")
        df = pd.read_csv(input.table, delimiter="\t", index_col=0)
        df["xgene"] = df["xgene"].astype(str)
        df["ygene"] = df["ygene"].astype(str)
        print(df)

        if params.man_or_auto == "manual":
            # parse the manual breaks in the config file
            print("  - Getting the manual breaks for x")
            xdf = determine_breaks(df, x_breaks, x_offset, "x", False)
            print("  - Getting the manual breaks for y")
            ydf = determine_breaks(df, y_breaks, y_offset, "y", False)
            # save the results to a file
            xdf.to_csv(output.xtable, sep="\t")
            ydf.to_csv(output.ytable, sep="\t")
        elif params.man_or_auto == "auto":
            # auto-determine the breaks if we need them
            print("  - Determining the new xbreaks automatically.")
            xdf = determine_breaks(df, x_breaks, x_offset, "x", True)
            print("  - Determining the new ybreaks automatically.")
            ydf = determine_breaks(df, y_breaks, y_offset, "y", True)
            # save the results to a file
            xdf.to_csv(output.xtable, sep="\t")
            ydf.to_csv(output.ytable, sep="\t")

def synteny_plot(plotting_df,    xcoords_file,  ycoords_file,
                 xsample,        ysample,
                 xbreaks_file,   ybreaks_file,
                 synplot,        prot_to_color,  dropmissing,
                 plot_x_lines = False,
                 plot_y_lines = False,
                 xprottoloc = False,
                 yprottoloc = False, dropNone = False):
    """
    If the user provided a plot order, then we should not skip any scaffolds.

    This is the main plotting script for the synteny plot
    """
    import pandas as pd
    import seaborn as sns; sns.set()
    import matplotlib
    import matplotlib.pyplot as plt
    import matplotlib.ticker as ticker
    #import matplotlib.patches as mplpatches
    from matplotlib.ticker import StrMethodFormatter, NullFormatter
    import numpy as np
    # set seaborn stuff
    #sns.set(rc={'text.usetex' : True})
    sns.set_style("ticks", {'font.family': ['sans-serif'],
                                'font.sans-serif': ['Helvetica'],
                                'grid.color': '.95'})
    # Preserve the vertical order of embedded images:
    matplotlib.rcParams['image.composite_image'] = False
    # text as font in pdf
    matplotlib.rcParams['pdf.fonttype'] = 42
    matplotlib.rcParams['ps.fonttype'] = 42

    # open the xbreaks and the ybreaks dataframes
    # These are where there are low-opacity,
    #  dotted lines to show breaks in synteny
    # example entries are like this:
    xbreaks_df = pd.read_csv(xbreaks_file, delimiter="\t", index_col=0)
    xbreaks_df["xgene"] = xbreaks_df["xgene"].astype(str)
    xbreaks_df["ygene"] = xbreaks_df["ygene"].astype(str)
    ybreaks_df = pd.read_csv(ybreaks_file, delimiter="\t", index_col=0)
    ybreaks_df["xgene"] = ybreaks_df["xgene"].astype(str)
    ybreaks_df["ygene"] = ybreaks_df["ygene"].astype(str)


    # first make a lookup table of how to calculate the
    #  x and y coords. This lookup is just the amount of
    # bp to add to the value when plotting. We pass the xprot_to_loc,
    #  xprot_to_scaf in case we need to sort everything based on order of
    #  occurrence on the scaffolds
    x_offset, x_scaf_to_len, vertical_lines_at, xmax, xticklabel, xtickpos, xorder = parse_coords(
        xcoords_file, xsample, "x", dropNone = dropNone)
    print("found {} x chromosomes".format(len(x_offset)))

    y_offset, y_scaf_to_len, horizontal_lines_at, ymax, yticklabel, ytickpos, yorder = parse_coords(
        ycoords_file, ysample, "y")
    print("found {} y chromosomes".format(len(y_offset)))

    # first make a lookup table
    df = pd.read_csv(plotting_df, delimiter="\t", index_col=0)
    df["xgene"] = df["xgene"].astype(str)
    df["ygene"] = df["ygene"].astype(str)

    xgene = df["xgene"]
    x = df["xmiddle"]
    y = df["ymiddle"]
    bitscore = df["bitscore"]

    print("found {} points to plot".format(len(df["xmiddle"])))
    print("max bitscore: ", max(bitscore))
    bitscore_adjusted = [(x/max(bitscore))*60 for x in bitscore]
    colors = [(0, 0, 1.0, min( 1.0, (x/max(bitscore))*(max(bitscore)/np.mean(bitscore)))) for x in bitscore]
    drops = set()
    if prot_to_color:
        for i in range(len(xgene)):
            alpha = colors[i][3]
            try:
                newcolor = list(matplotlib.colors.to_rgba(prot_to_color[xgene[i]]))
            except:
                print("couldn't find a color for: {}".format(xgene[i]))
                newcolor = [0,0,0,1]
                drops.add(i)
            newcolor[3] = alpha
            colors[i] = newcolor

    if dropmissing:
        vararray = [xgene, x, y, bitscore, bitscore_adjusted, colors]
        for j in range(len(vararray)):
            vararray[j] = [vararray[j][i] for i in range(len(vararray[j])) if i not in drops]
        xgene = vararray[0]
        x     = vararray[1]
        y     = vararray[2]
        bitscore = vararray[3]
        bitscore_adjusted = vararray[4]
        colors = vararray[5]

    # now make a scatter plot
    figWidth = 8
    figHeight = 8
    plt.figure(figsize=(figWidth,figHeight))
    #set the panel dimensions
    panelWidth = 4
    panelHeight = 4
    dpanel_width = 0.25
    #find the margins to center the panel in figure
    leftMargin = (figWidth - panelWidth)/2
    bottomMargin = ((figHeight - panelHeight)/2)
    panel1 = plt.axes([leftMargin/figWidth, #left
                         bottomMargin/figHeight,    #bottom
                         panelWidth/figWidth,   #width
                         panelHeight/figHeight])     #height
    panelxd = plt.axes([leftMargin/figWidth, #left
                         (bottomMargin+panelHeight+0.1)/figHeight,    #bottom
                         panelWidth/figWidth,   #width
                         dpanel_width/figHeight])     #height
    panelyd = plt.axes([(leftMargin+panelWidth + 0.1)/figWidth, #left
                         bottomMargin/figHeight,    #bottom
                         dpanel_width/figWidth,   #width
                         panelHeight/figHeight])     #height
    panel1.tick_params(axis='both',which='both',
                        bottom=False, labelbottom=True,
                        left=False, labelleft=True,
                        right=False, labelright=False,
                        top=False, labeltop=False)
    panelxd.tick_params(axis='both',which='both',
                        bottom=False, labelbottom=False,
                        left=False, labelleft=False,
                        right=False, labelright=False,
                        top=False, labeltop=False)
    panelyd.tick_params(axis='both',which='both',
                        bottom=False, labelbottom=False,
                        left=False, labelleft=False,
                        right=False, labelright=False,
                        top=False, labeltop=False)
    # set the panel linewidth thinner
    for this_panel in [panel1, panelxd, panelyd]:
        for axis in ['top','bottom','left','right']:
            this_panel.spines[axis].set_linewidth(0.5)
    # turn off the axis spines
    for this_panel in [panelxd, panelyd]:
        this_panel.spines['top'].set_visible(False)
        this_panel.spines['right'].set_visible(False)

    panel1.scatter(x, y, color = colors,
                   ec = None, s=6, linewidths = 0)
    print("xmax is")
    print(xmax)
    # set mins and max
    panel1.set_xlim([0, xmax])
    panel1.set_ylim([0, ymax])

    # set x ticks
    newarr = []
    newarrlabels=[]
    if not plot_x_lines:
        #there are inevitably going to be many scaffolds. We need to subset
        # get a list of evenly spaced indices
        numElems = min(20, len(xtickpos)) # this could break if there are fewer elements
        arr = xtickpos
        idx = np.round(np.linspace(0, len(arr) - 1, numElems)).astype(int)
        newarr       = [arr[i] for i in idx]
        newarrNumScaf     = [i for i in idx]
        # turn on y-axis ticks on the left - plot scaffolds
        panel1.tick_params(bottom=True)
        panel1.set_xticks(newarr)
        panel1.set_xticklabels(newarrNumScaf, fontsize=8, rotation=90)
        panel1.set_xlabel(xsample + " number of scaffolds")
    else:
        newarr = [0] + list(x_offset.values())
        panel1.set_xticks(xtickpos)
        panel1.set_xticklabels(xticklabel, fontsize=8, rotation = 90)
        panel1.set_xlabel(xsample + " scaffolds")
    # turn on x-axis ticks on the Dx plot
    newarrlabels = [round(x/1000000, 1) for x in newarr]
    panelxd.tick_params(top=True, labeltop=True)
    panelxd.set_xticks(newarr)
    panelxd.set_xticklabels(newarrlabels, fontsize=8, rotation=90)
    panelxd.xaxis.set_label_position("top")
    panelxd.set_xlabel("Mb")

    # set y ticks
    newarr=[]
    newarrlabels=[]
    if not plot_y_lines:
        #there are inevitably going to be many scaffolds. We need to subset
        # get a list of evenly spaced indices
        numElems = min(20, len(ytickpos))
        arr = ytickpos
        idx = np.round(np.linspace(0, len(arr) - 1, numElems)).astype(int)
        newarr       = [arr[i] for i in idx]
        newarrlabels = [round(arr[i]/1000000, 1) for i in idx]
        newarrNumScaf     = [i for i in idx]
        # turn on y-axis ticks on the left - plot scaffolds
        panel1.tick_params(left=True)
        panel1.set_yticks(newarr)
        panel1.set_yticklabels(newarrNumScaf, fontsize=8)
        panel1.set_ylabel(ysample + " number of scaffolds")
    else:
        newarr = [0] + horizontal_lines_at + [ymax]
        panel1.set_yticks(ytickpos)
        panel1.set_yticklabels(yticklabel, fontsize=8)
        panel1.set_ylabel(ysample + " scaffolds")
    # turn on y-axis ticks on the Dy plot
    newarrlabels = [round(x/1000000, 1) for x in newarr]
    panelyd.tick_params(right=True, labelright=True)
    panelyd.set_yticks(newarr)
    panelyd.set_yticklabels(newarrlabels, fontsize=8)
    panelyd.yaxis.set_label_position("right")
    panelyd.set_ylabel("Mb")

    # set the x and y labels on Dy and Dx
    panelxd.bar(x = df["Dx_barmiddle"], height=df["Dx"], width = df["Dx_barwidth"],
                align = "center", lw=0, color="blue", zorder = 2)
    panelxd.set_xlim([0,xmax])
    panelxd.set_ylabel('Dx', fontsize=10)

    panelyd.barh(y = df["Dy_barmiddle"], width=df["Dy"], height = df["Dy_barwidth"],
                 align = "center", lw=0, color="blue", zorder = 2)
    panelyd.set_ylim([0,ymax])
    panelyd.set_xlabel('Dy', fontsize=10)

    for this_axis in [panel1, panelxd, panelyd]:
        this_axis.xaxis.get_offset_text().set_visible(False)
        this_axis.yaxis.get_offset_text().set_visible(False)

    #plot vertical lines
    if plot_x_lines:
        for value in vertical_lines_at:
            panel1.axvline(x=value, color="black", lw=0.5)
    #plot horizontal lines
    if plot_y_lines:
        for value in horizontal_lines_at:
            panel1.axhline(y=value, color="black", lw=0.5)

    # plot vertical BOS
    for value in xbreaks_df["xmiddle"]:
        panel1.axvline(x=value, color=[0,0,0,0.25], lw=0.5, linestyle="dotted")
    # plot horizontal BOS
    for value in ybreaks_df["ymiddle"]:
        panel1.axhline(y=value, color=[0,0,0,0.25], lw=0.5, linestyle="dotted")
    plt.savefig(synplot)


"""
This makes the synteny plot without doing any special coloring of the dots
"""
rule plot_synteny:
    input:
        dtable = "synteny_analysis/dvalue_table/{xsample}_and_{ysample}_info.tsv",
        plot_order = "synteny_analysis/plot_order/{xsample}_and_{ysample}_plotorder.tsv",
        ycoords = "synteny_analysis/genome_coords/y_genome_coords/{ysample}_genomecoords.txt",
        xcoords = "synteny_analysis/genome_coords/x_genome_coords/{xsample}_genomecoords.txt",
        xbreaks_auto = "synteny_analysis/dvalue_table_breaks/{xsample}_and_{ysample}_xbreaks_auto.tsv",
        ybreaks_auto = "synteny_analysis/dvalue_table_breaks/{xsample}_and_{ysample}_ybreaks_auto.tsv",
        xbreaks_manual = "synteny_analysis/dvalue_table_breaks/{xsample}_and_{ysample}_xbreaks_manual.tsv",
        ybreaks_manual = "synteny_analysis/dvalue_table_breaks/{xsample}_and_{ysample}_ybreaks_manual.tsv",
        xprottoloc = "unwrapped_RBH/RBH/reciprocal_best_hits.chrom",
        yprottoloc = lambda wildcards: config["yaxisspecies"][wildcards.ysample]["prot_to_loc"],
        recip = "unwrapped_RBH/hmm/searches_agg_best/{ysample}_hmm_best.tsv"
    output:
        synplot_manual = "synteny_analysis/plots/synteny_uncolored/{xsample}_and_{ysample}_synteny_manualbreaks.pdf",
        synplot_noNone = "synteny_analysis/plots/synteny_uncolored/{xsample}_and_{ysample}_synteny_manualbreaks_noNone.pdf",
    threads:
        1
    params:
        xsample  = lambda wildcards: wildcards.xsample,
        ysample  = lambda wildcards: wildcards.ysample,
        keep_x   = True,
        keep_y   = lambda wildcards: False if ( ("sort_by_x_coord_blast" in config["yaxisspecies"][wildcards.ysample]) or ("noylines" in config["yaxisspecies"][wildcards.ysample])) else True,
    run:
        # plot with None ALG
        synteny_plot(input.dtable,   input.xcoords,  input.ycoords,
                     params.xsample, params.ysample,
                     input.xbreaks_manual, input.ybreaks_manual,
                     output.synplot_manual, None,           False,
                     plot_x_lines = params.keep_x,
                     plot_y_lines = params.keep_y,
                     dropNone = False)
        # plot without None ALG
        synteny_plot(input.dtable,   input.xcoords,  input.ycoords,
                     params.xsample, params.ysample,
                     input.xbreaks_manual, input.ybreaks_manual,
                     output.synplot_noNone, None,           False,
                     plot_x_lines = params.keep_x,
                     plot_y_lines = params.keep_y,
                     dropNone = True)


def sizes_and_scale_minp(smallest_p):
    """
    This outputs two lists. One of scale, and one of sizes,
    based on the smallest_p.

    This is used for plotting elipses in the figure and the legend
    """
    scale     = [1,       0.84,     0.67,     0.5,    0.3   ]

    if smallest_p == 0:
        sizes = [0,       "1E-20", "1E-10", "1E-5", "1E-2"]
    elif smallest_p <= float("1E-20"):
        sizes = ["1E-20", "1E-15", "1E-7",  "1E-4", "5E-2"]
    elif smallest_p <= float("1E-15"):
        sizes = ["1E-15", "1E-10", "1E-5",  "1E-2", "1E-1"]
    elif smallest_p <= float("1E-10"):
        sizes = ["1E-10", "1E-7",  "1E-4",  "1E-2", "1E-1"]
    elif smallest_p <= float("1E-8"):
        sizes = ["1E-8",  "1E-6",  "1E-4",  "1E-2", "1E-1"]
    elif smallest_p <= float("1E-6"):
        sizes = ["1E-6",  "1E-5",  "1E-3",  "1E-2", "1E-1"]
    elif smallest_p <= float("1E-5"):
        sizes = ["1E-5",  "1E-4",  "1E-3",  "1E-2", "0.5"]
    else:
        sizes = ["1E-4",  "1E-3",  "1E-2",  "1E-1", "0.5"]
    return [scale, sizes]

def quantize_pvalue(xsize, ysize, p, smallest_p):
    """
    This quantizes the pvalues using standard values cutoffs
    returns [xsize, ysize]
    """
    scale, sizes = sizes_and_scale_minp(smallest_p)
    for i in range(len(sizes)):
        if p <= float(sizes[i]):
            return [xsize * scale[i], ysize * scale[i]]
    return [xsize * 0.15, ysize * 0.15]

def fishers_exact(plotting_df,
                  xcoords_file, ycoords_file,
                  xsample,      ysample,
                  xbreaks_file, ybreaks_file,
                  outdf, style):
    """
    This test performs fishers exact test on the genome based on reciprocal
     best hits, and breaks.

    There are two styles, "whole" & "breaks"
      - "whole" is conducted by performing Fisher's exact on whole scaffolds
      - "breaks" is conducted by performing the test on pieces of scaffolds
    """
    from scipy import stats
    # open the xbreaks and the ybreaks dataframes
    # These are where there are low-opacity,
    #  dotted lines to show breaks in synteny
    # example entries are like this:
    xbreaks_df = pd.read_csv(xbreaks_file, delimiter="\t", index_col=0)
    ybreaks_df = pd.read_csv(ybreaks_file, delimiter="\t", index_col=0)

    # first make a lookup table of how to calculate the
    #  x and y coords. This lookup is just the amount of
    # bp to add to the value when plotting. We pass the xprot_to_loc,
    #  xprot_to_scaf in case we need to sort everything based on order of
    #  occurrence on the scaffolds
    x_offset, x_scaf_to_len, vertical_lines_at, xmax, xticklabel, xtickpos, xorder = parse_coords(
        xcoords_file, xsample, "x")
    print("found {} x chromosomes".format(len(x_offset)))

    y_offset, y_scaf_to_len, horizontal_lines_at, ymax, yticklabel, ytickpos, yorder = parse_coords(
        ycoords_file, ysample, "y")

    # first make a lookup table
    df = pd.read_csv(plotting_df, delimiter="\t", index_col=0)
    df["xgene"] = df["xgene"].astype(str)
    df["ygene"] = df["ygene"].astype(str)

    fisher_dict = []
    if style == "breaks":
        xranges_to_dfdict = {}
        yranges_to_dfdict = {}
        print("generating breaks dfs for x scaffolds")
        for xscaf in xorder:
            plot_xmin  = x_offset[xscaf]
            plot_xmax  = x_offset[xscaf] + x_scaf_to_len[xscaf]
            subxdf = xbreaks_df.loc[xbreaks_df["xscaf"] == xscaf, ]
            subxdf = subxdf.sort_values(by=["xstop"])
            xbreaks = [plot_xmin] + list(subxdf["xstop"]) + [plot_xmax]
            for xi in range(len(xbreaks)-1):
                plotleft   = xbreaks[xi]
                plotright  = xbreaks[xi+1]
                query = "xmiddle >= {} & xmiddle < {}".format(plotleft, plotright)
                inxdf  = df.query(query)
                outxdf = df[~df.isin(inxdf).all(1)]
                interval = "{}:{}-{}".format(xscaf, plotleft, plotright)
                xranges_to_dfdict[interval] = {"in": inxdf, "out": outxdf}
        print("generating breaks dfs for y scaffolds")
        for yscaf in yorder:
            plot_ymin  = y_offset[yscaf]
            plot_ymax  = y_offset[yscaf] + y_scaf_to_len[yscaf]
            subydf = ybreaks_df.loc[ybreaks_df["yscaf"] == yscaf, ]
            subydf = subydf.sort_values(by=["ystop"])
            ybreaks = [plot_ymin] + list(subydf["ystop"]) + [plot_ymax]
            for yi in range(len(ybreaks)-1):
                plotbottom = ybreaks[yi]
                plottop    = ybreaks[yi+1]
                query = "ymiddle >= {} & ymiddle < {}".format(plotbottom, plottop)
                inydf  = df.query(query)
                outydf = df[~df.isin(inydf).all(1)]
                interval = "{}:{}-{}".format(yscaf, plotbottom, plottop)
                yranges_to_dfdict[interval] = {"in": inydf, "out": outydf}
        # now that we have each interval and their respective dfs, we
        #  iterate through everything and make the fisher's exact test entry
        counter = 0
        num_variations = len(xranges_to_dfdict) * len(yranges_to_dfdict)
        for xinterval in xranges_to_dfdict:
            for yinterval in yranges_to_dfdict:
                counter += 1
                print("\r    Completed {}/{} interval calculations   ".format(counter, num_variations), end = "")
                xscaf     = xinterval.split(":")[0]
                plotleft  = int(float(xinterval.split(":")[1].split("-")[0]))
                plotright = int(float(xinterval.split(":")[1].split("-")[1]))
                plotwidth  = plotright - plotleft
                yscaf     = yinterval.split(":")[0]
                plotbottom  = int(float(yinterval.split(":")[1].split("-")[0]))
                plottop     = int(float(yinterval.split(":")[1].split("-")[1]))
                plotheight = plottop - plotbottom
                inxdf     = xranges_to_dfdict[xinterval]["in"]
                outxdf     = xranges_to_dfdict[xinterval]["out"]
                inydf     = yranges_to_dfdict[yinterval]["in"]
                outydf     = yranges_to_dfdict[yinterval]["out"]
                inX_inY   = len(inxdf.merge(inydf))
                inX_outY  = len(inxdf.merge(outydf))
                outX_inY  = len(outxdf.merge(inydf))
                outX_outY = len(outxdf.merge(outydf))
                table = [[inX_inY, outX_inY], [inX_outY, outX_outY]]
                oddsratio, pvalue = stats.fisher_exact(table, alternative="greater")
                fisher_dict.append({"xscaf": xscaf, "yscaf": yscaf,
                                    "inX_inY": inX_inY, "inX_outY": inX_outY,
                                    "outX_inY": outX_inY, "outX_outY": outX_outY,
                                    "oddsratio": oddsratio, "pvalue": pvalue,
                                    "plotleft": plotleft,
                                    "plotright": plotright,
                                    "plotwidth": plotwidth,
                                    "plotbottom": plotbottom,
                                    "plottop": plottop,
                                    "plotheight": plotheight,
                                    "x_plot_pos": xtickpos[xticklabel.index(xscaf)],
                                    "y_plot_pos": ytickpos[yticklabel.index(yscaf)]})

    if style == "whole": # perform the analysis on whole chromosomes
        for xscaf in xorder: #Li
            for yscaf in yorder: #Wi
                #calculate the cells for fisher exact
                plotleft = x_offset[xscaf]
                plotright = x_offset[xscaf] + x_scaf_to_len[xscaf]
                plotwidth = plotright - plotleft
                plotbottom = y_offset[yscaf]
                plottop    = y_offset[yscaf] + y_scaf_to_len[yscaf]
                plotheight = plottop-plotbottom
                query = "yscaf == '{}' & xscaf == '{}'".format(yscaf, xscaf)
                inX_inY   = len(df.query(query))
                query = "yscaf == '{}' & xscaf != '{}'".format(yscaf, xscaf)
                inX_outY  = len(df.query(query))
                query = "yscaf != '{}' & xscaf == '{}'".format(yscaf, xscaf)
                outX_inY  = len(df.query(query))
                query = "yscaf != '{}' & xscaf != '{}'".format(yscaf, xscaf)
                outX_outY = len(df.query(query))
                table = [[inX_inY, outX_inY], [inX_outY, outX_outY]]
                oddsratio, pvalue = stats.fisher_exact(table, alternative="greater")
                fisher_dict.append({"xscaf": xscaf, "yscaf": yscaf,
                                    "inX_inY": inX_inY, "inX_outY": inX_outY,
                                    "outX_inY": outX_inY, "outX_outY": outX_outY,
                                    "oddsratio": oddsratio, "pvalue": pvalue,
                                    "plotleft": plotleft,
                                    "plotright": plotright,
                                    "plotwidth": plotwidth,
                                    "plotbottom": plotbottom,
                                    "plottop": plottop,
                                    "plotheight": plotheight,
                                    "x_plot_pos": xtickpos[xticklabel.index(xscaf)],
                                    "y_plot_pos": ytickpos[yticklabel.index(yscaf)]})
    fisherdf = pd.DataFrame(fisher_dict)
    fisherdf.to_csv(outdf, sep="\t")

rule gen_fishertable_whole:
    """
    generates a table of fisher's exact test results for whole chromosomes
    output is a df that can be read back into pandas
    """
    input:
        dtable = "synteny_analysis/dvalue_table/{xsample}_and_{ysample}_info.tsv",
        ycoords = "synteny_analysis/genome_coords/y_genome_coords/{ysample}_genomecoords.txt",
        xcoords = "synteny_analysis/genome_coords/x_genome_coords/{xsample}_genomecoords.txt",
        xbreaks_manual = "synteny_analysis/dvalue_table_breaks/{xsample}_and_{ysample}_xbreaks_manual.tsv",
        ybreaks_manual = "synteny_analysis/dvalue_table_breaks/{xsample}_and_{ysample}_ybreaks_manual.tsv",
    output:
        fisherswhole   = "synteny_analysis/plots/significance/tables/{xsample}_and_{ysample}_fisher_wholechr.tsv"
    threads:
        1
    params:
        xsample = lambda wildcards: wildcards.xsample,
        ysample = lambda wildcards: wildcards.ysample,
        style   = "whole",
    run:
        fishers_exact(input.dtable,
                      input.xcoords,         input.ycoords,
                      params.xsample,        params.ysample,
                      input.xbreaks_manual,  input.ybreaks_manual,
                      output.fisherswhole,   params.style)

rule gen_fishertable_manualbreaks:
    """
    generates a table of fisher's exact test results for the manual breaks chromosomes
    output is a df that can be read back into pandas
    """
    input:
        dtable = "synteny_analysis/dvalue_table/{xsample}_and_{ysample}_info.tsv",
        ycoords = "synteny_analysis/genome_coords/y_genome_coords/{ysample}_genomecoords.txt",
        xcoords = "synteny_analysis/genome_coords/x_genome_coords/{xsample}_genomecoords.txt",
        xbreaks_manual = "synteny_analysis/dvalue_table_breaks/{xsample}_and_{ysample}_xbreaks_manual.tsv",
        ybreaks_manual = "synteny_analysis/dvalue_table_breaks/{xsample}_and_{ysample}_ybreaks_manual.tsv",
    output:
        fishersmanualbreaks   = "synteny_analysis/plots/significance/tables/{xsample}_and_{ysample}_fisher_manualbreaks.tsv"
    threads:
        1
    params:
        xsample = lambda wildcards: wildcards.xsample,
        ysample = lambda wildcards: wildcards.ysample,
        style   = "breaks",
    run:
        fishers_exact(input.dtable,
                      input.xcoords,         input.ycoords,
                      params.xsample,        params.ysample,
                      input.xbreaks_manual,  input.ybreaks_manual,
                      output.fishersmanualbreaks,   params.style)

rule gen_fishertable_autobreaks:
    """
    generates a table of fisher's exact test results for the manual breaks chromosomes
    output is a df that can be read back into pandas
    """
    input:
        dtable = "synteny_analysis/dvalue_table/{xsample}_and_{ysample}_info.tsv",
        ycoords = "synteny_analysis/genome_coords/y_genome_coords/{ysample}_genomecoords.txt",
        xcoords = "synteny_analysis/genome_coords/x_genome_coords/{xsample}_genomecoords.txt",
        xbreaks_auto = "synteny_analysis/dvalue_table_breaks/{xsample}_and_{ysample}_xbreaks_auto.tsv",
        ybreaks_auto = "synteny_analysis/dvalue_table_breaks/{xsample}_and_{ysample}_ybreaks_auto.tsv",
    output:
        fishersautobreaks   = "synteny_analysis/plots/significance/tables/{xsample}_and_{ysample}_fisher_autobreaks.tsv"
    threads:
        1
    params:
        xsample = lambda wildcards: wildcards.xsample,
        ysample = lambda wildcards: wildcards.ysample,
        style   = "breaks",
    run:
        fishers_exact(input.dtable,
                      input.xcoords,         input.ycoords,
                      params.xsample,        params.ysample,
                      input.xbreaks_auto,  input.ybreaks_auto,
                      output.fishersautobreaks,   params.style)

def scale_pvalue_to_0_1(vector, reverse = False):
    """
    scales a value to between 0 and 1 given the min and max values seen in that vector

    we might pass the log, so log(0) = inf. Look for that as special case to define minp

    returns a vector
    """
    #first find max
    maxp = -99999999999999
    minp = 99999999999
    for entry in vector:
        if math.isinf(entry) or math.isnan(entry):
            pass
        elif entry < minp:
            minp = entry
        elif entry > maxp:
            maxp = entry
    # now scale everything to between 0 and 1
    outvec = []
    for entry in vector:
        corrected = entry
        if math.isinf(entry) or math.isnan(entry):
            corrected = sys.float_info.min
        thisval = (corrected-minp)/(maxp-minp)
        if reverse:
            outvec.append(1-thisval)
        else:
            outvec.append(thisval)
    return outvec

def correlation_plot(plotting_df, fishertsv,
                     xcoords_file, ycoords_file,
                     xsample,      ysample,
                     xbreaks_file, ybreaks_file,
                     densityplot, style, autoscale_color,
                     dropNone = False):
    """
    correlation plot makes a plot of the Fisher's exact test for each chromosome

    The style variable can be:
      - "whole_dots"
      - "whole_colors"
      - "breaks"
    """
    gray   = "#d1cbcc"
    yellow = "#f3e056"
    orange = "#f98e08"
    red    = "#7d1d6c"
    from scipy import stats
    import pandas as pd
    import seaborn as sns; sns.set()
    import matplotlib
    import matplotlib.pyplot as plt
    import matplotlib.ticker as ticker
    from matplotlib.ticker import StrMethodFormatter, NullFormatter
    from matplotlib import cm
    from matplotlib.colors import ListedColormap, LinearSegmentedColormap
    import numpy as np
    # set seaborn stuff
    #sns.set(rc={'text.usetex' : True})
    sns.set_style("ticks", {'font.family': ['sans-serif'],
                                'font.sans-serif': ['Helvetica'],
                                'grid.color': '.95'})
    # Preserve the vertical order of embedded images:
    matplotlib.rcParams['image.composite_image'] = False
    # text as font in pdf
    matplotlib.rcParams['pdf.fonttype'] = 42
    matplotlib.rcParams['ps.fonttype'] = 42

    # open the xbreaks and the ybreaks dataframes
    # These are where there are low-opacity,
    #  dotted lines to show breaks in synteny
    # example entries are like this:
    xbreaks_df = pd.read_csv(xbreaks_file, delimiter="\t", index_col=0)
    ybreaks_df = pd.read_csv(ybreaks_file, delimiter="\t", index_col=0)

    # first make a lookup table of how to calculate the
    #  x and y coords. This lookup is just the amount of
    # bp to add to the value when plotting. We pass the xprot_to_loc,
    #  xprot_to_scaf in case we need to sort everything based on order of
    #  occurrence on the scaffolds
    x_offset, x_scaf_to_len, vertical_lines_at, xmax, xticklabel, xtickpos, xorder = parse_coords(
        xcoords_file, xsample, "x", dropNone = dropNone)
    print("found {} x chromosomes".format(len(x_offset)))

    y_offset, y_scaf_to_len, horizontal_lines_at, ymax, yticklabel, ytickpos, yorder = parse_coords(
        ycoords_file, ysample, "y")

    # first make a lookup table
    df       = pd.read_csv(plotting_df, delimiter="\t", index_col=0)
    fisherdf = pd.read_csv(fishertsv, delimiter="\t", index_col=0)
    fisherdf["pval01"] = scale_pvalue_to_0_1(fisherdf["pvalue"], reverse = False)
    print(fisherdf["pval01"])
    fisherdf["pval01_rev"] = scale_pvalue_to_0_1(fisherdf["pvalue"], reverse = True)
    fisherdf["pval01_log"] = scale_pvalue_to_0_1(np.log(fisherdf["pvalue"]), reverse = False)
    fisherdf["pval01_log_rev"] = scale_pvalue_to_0_1(np.log(fisherdf["pvalue"]), reverse = True)

    print("Performed {} tests".format(len(fisherdf)))
    print("0.05 equivalent for {} tests is {}".format(len(fisherdf), 0.05/len(fisherdf)))
    p_value_dict = {x: x/len(fisherdf) for x in [0.05, 0.00005, 0.00000005]}
    p_value_to_color = {0.05:       yellow,
                        0.00005:    orange,
                        0.00000005: red}
    fisherdf["color"] = gray
    # now add colors to each entry in the dataframe
    for index, row in fisherdf.iterrows():
        thisp      = row["pvalue"]
        min_vals   = [key for key in p_value_dict if thisp <= p_value_dict[key]]
        if len(min_vals) > 0:
            this_color = p_value_to_color[min(min_vals)]
            fisherdf.loc[index, "color"] = this_color
    for key in p_value_dict:
        subdf = fisherdf.loc[fisherdf["pvalue"] < p_value_dict[key], ]
        print("There are {} cells with pvalues below the {} Bf-corrected pvalue, {}".format(
            len(subdf), key, p_value_dict[key]))
    # figure out the widths of the cells to find the optimal dot size
    widths = []
    #for this_list in [horizontal_lines_at, vertical_lines_at
    for this_list in [list(sorted(x_offset.values()))]:
        for i in range(len(this_list)-1):
            widths.append(this_list[i+1]-this_list[i])
    x_median_size = statistics.median(sorted(widths))
    x_median_size = np.percentile(sorted(widths), 85)
    max_x = max(x_offset.values())
    max_y = max(y_offset.values())
    y_median_size = (x_median_size/max_x)*max_y

    # now make a scatter plot
    figWidth = 8
    figHeight = 8
    plt.figure(figsize=(figWidth,figHeight))
    #set the panel dimensions
    panelWidth = 4
    panelHeight = 4
    dpanel_width = 0.25
    #find the margins to center the panel in figure
    leftMargin = (figWidth - panelWidth)/2
    bottomMargin = ((figHeight - panelHeight)/2)
    panel1 = plt.axes([leftMargin/figWidth, #left
                         bottomMargin/figHeight,    #bottom
                         panelWidth/figWidth,   #width
                         panelHeight/figHeight])     #height
    panel1.tick_params(axis='both',which='both',
                        bottom=False, labelbottom=True,
                        left=False, labelleft=True,
                        right=False, labelright=False,
                        top=False, labeltop=False)
    panelleg = plt.axes([(leftMargin+panelWidth + 0.1)/figWidth, #left
                         bottomMargin/figHeight,    #bottom
                         (dpanel_width*4)/figWidth,   #width
                         panelHeight/figHeight])     #height
    panelleg.tick_params(axis='both',which='both',
                        bottom=False, labelbottom=False,
                        left=False, labelleft=False,
                        right=False, labelright=False,
                        top=False, labeltop=False)
    smallest_p = min(fisherdf["pvalue"])
    if style == "whole_dots":
        # plot the circles for the significance
        for index, row in fisherdf.iterrows():
            #x, y = quantize_pvalue(x_median_size/2,
            #                      y_median_size/2,
            #                      float(row["pvalue"]),
            #                      smallest_p)
            x = x_median_size/2
            y = y_median_size/2
            circ = matplotlib.patches.Ellipse(
                       (row["x_plot_pos"], row["y_plot_pos"]),
                       x, y, ec = None, lw = None, color=row["color"])
            panel1.add_patch(circ)
    elif style in ["whole_colors", "breaks"]:
        magma = cm.get_cmap("magma", 8)
        #magma = cm.get_cmap('magma', 8)
        #newcolors = magma(np.linspace(0, 1, 12))
        #white = np.array([256/256, 256/256, 256/256, 1])
        #newcolors[:5, :] = white
        #newcmp = ListedColormap(newcolors)
        for index, row in fisherdf.iterrows():
            left   = row["plotleft"]
            bottom = row["plotbottom"]
            width  = row["plotwidth"]
            height = row["plotheight"]
            if autoscale_color:
                rgba = magma(row["pval01_log"])
                rectangle = matplotlib.patches.Rectangle((left, bottom), width, height,
                                                  linewidth=0,
                                                  facecolor=rgba,
                                                  edgecolor="black", zorder = 2)
            else:
                rectangle = matplotlib.patches.Rectangle((left, bottom), width, height,
                                                  linewidth=0,
                                                  facecolor=row["color"],
                                                  edgecolor="black", zorder = 2)
            panel1.add_patch(rectangle)

    # set the panel linewidth thinner
    for this_panel in [panel1]:
        for axis in ['top','bottom','left','right']:
            this_panel.spines[axis].set_linewidth(0.5)

    # remove spines on legend
    for this_panel in [panelleg]:
        for axis in ['top','bottom','left','right']:
            this_panel.spines[axis].set_visible(False)

    # set mins and max
    panel1.set_xlim([0,xmax])
    panel1.set_ylim([0,ymax])
    xscalar = (dpanel_width*4)/panelWidth
    panelleg.set_xlim([0,xmax*xscalar]) # percent
    panelleg.set_ylim([0,ymax])

    # now plot the legend
    # there are 6 sizes to plot
    # take up the top 1/3 of the panel
    if style == "circles":
        legend_bottom_y = ymax - int(ymax/3)
        legend_top_y    = ymax
        leg_y_seglen        = int(ymax/3/7)
        # set the points 15% away from left edge
        leg_x_offset        = 0.15 * (xmax*xscalar)
        leg_x_pos_ellipse = [leg_x_offset]*6
        leg_y_pos_ellipse = [legend_top_y - leg_y_seglen]
        for i in range(6):
            leg_y_pos_ellipse.append(leg_y_pos_ellipse[-1] - leg_y_seglen)
        # now actually plot the ellipses
        legscale, legsize = sizes_and_scale_minp(smallest_p)
        legscale.append(0.15)
        legsize.append("1")
        for i in range(6):
            x, y = quantize_pvalue(x_median_size/2,
                                  y_median_size/2,
                                  float(legsize[i]),
                                  min([float(x) for x in legsize]))
            circ = matplotlib.patches.Ellipse(
                       (leg_x_pos_ellipse[i], leg_y_pos_ellipse[i]),
                       x, y, ec = None, lw = None, color="gray")
            panelleg.add_patch(circ)
            textx = leg_x_pos_ellipse[i] + leg_x_offset
            texty = leg_y_pos_ellipse[i]
            s = "p <= {}".format(legsize[i])
            matplotlib.pyplot.text(textx, texty, s, ha="left", va = "center", fontsize = 8)
    elif (("whole" in style) or (style == "breaks")):
        legend_bottom_y = ymax - int(ymax/3)
        legend_top_y    = ymax
        leg_y_seglen        = int(ymax/3/7)
        # set the points 15% away from left edge
        leg_x_offset        = 0.15 * (xmax*xscalar)
        leg_x_pos_ellipse = [leg_x_offset]*6
        leg_y_pos_ellipse = [legend_top_y - leg_y_seglen]
        for i in range(4):
            leg_y_pos_ellipse.append(leg_y_pos_ellipse[-1] - leg_y_seglen)
        # now actually plot the ellipses
        legscale, legsize = sizes_and_scale_minp(smallest_p)
        legscale.append(0.15)
        legsize.append("1")
        index_to_color = [gray, yellow, orange, red]
        index_to_text  = ["> 0.05", "< 0.05", "< 5e-4", "< 5e-7"]
        for i in range(4):
            x = x_median_size/2
            y = y_median_size/2
            circ = matplotlib.patches.Ellipse(
                       (leg_x_pos_ellipse[i], leg_y_pos_ellipse[i]),
                       x, y, ec = None, linewidth = None, color=index_to_color[i])
            panelleg.add_patch(circ)
            textx = leg_x_pos_ellipse[i] + leg_x_offset
            texty = leg_y_pos_ellipse[i]
            s = "{}".format(index_to_text[i])
            matplotlib.pyplot.text(textx, texty, s, ha="left", va = "center", fontsize = 8)

    # set x ticks
    panel1.set_xticks(xtickpos)
    panel1.set_xticklabels(xticklabel, fontsize=8, rotation = 90)
    panel1.set_xlabel(xsample + " scaffolds")

    # set y ticks
    panel1.set_yticks(ytickpos)
    panel1.set_yticklabels(yticklabel, fontsize=8)
    panel1.set_ylabel(ysample + " scaffolds")

    for this_axis in [panel1]:
        this_axis.xaxis.get_offset_text().set_visible(False)
        this_axis.yaxis.get_offset_text().set_visible(False)

    #plot vertical lines
    for value in vertical_lines_at:
        panel1.axvline(x=value, color="black", lw=0.5)
    #plot horizontal lines
    for value in horizontal_lines_at:
        panel1.axhline(y=value, color="black", lw=0.5)

    plt.savefig(densityplot)

rule plot_fishers_whole_color_cells:
    """
    Plots the correlation matrices of the whole scaffolds.
      - Colors in the whole cells.
    """

    input:
        dtable = "synteny_analysis/dvalue_table/{xsample}_and_{ysample}_info.tsv",
        fishertsv   = "synteny_analysis/plots/significance/tables/{xsample}_and_{ysample}_fisher_wholechr.tsv",
        ycoords = "synteny_analysis/genome_coords/y_genome_coords/{ysample}_genomecoords.txt",
        xcoords = "synteny_analysis/genome_coords/x_genome_coords/{xsample}_genomecoords.txt",
        xbreaks_manual = "synteny_analysis/dvalue_table_breaks/{xsample}_and_{ysample}_xbreaks_manual.tsv",
        ybreaks_manual = "synteny_analysis/dvalue_table_breaks/{xsample}_and_{ysample}_ybreaks_manual.tsv",
    output:
        density_plot   = "synteny_analysis/plots/significance/wholechr_colors/{xsample}_and_{ysample}_fisher_chromcolor.pdf",
        density_plot_noNone   = "synteny_analysis/plots/significance/wholechr_colors/{xsample}_and_{ysample}_fisher_chromcolor_noNone.pdf"
    threads:
        1
    params:
        xsample = lambda wildcards: wildcards.xsample,
        ysample = lambda wildcards: wildcards.ysample,
        style   = "whole_colors",
        autoscale = False
    run:
        # with None
        correlation_plot(input.dtable,          input.fishertsv,
                         input.xcoords,         input.ycoords,
                         params.xsample,        params.ysample,
                         input.xbreaks_manual,  input.ybreaks_manual,
                         output.density_plot,   params.style,
                         params.autoscale, dropNone = False)
        # with no None
        correlation_plot(input.dtable,          input.fishertsv,
                         input.xcoords,         input.ycoords,
                         params.xsample,        params.ysample,
                         input.xbreaks_manual,  input.ybreaks_manual,
                         output.density_plot_noNone,   params.style,
                         params.autoscale, dropNone = True)

rule xprot_to_color:
    """
    In addition to plotting the synteny without a color scheme,
      we also would like to plot by coloring with another species' color scheme
    The output is just:
    xsample_prot\thex_color
    """
    input:
        x_prot_to_loc = lambda wildcards: config["xaxisspecies"][wildcards.colorby]["prot_to_loc"]
    output:
        prot_to_color = "synteny_analysis/prot_to_color/{colorby}_prottocolor.tsv"
    params:
        colormap = lambda wildcards: config["xaxisspecies"][wildcards.colorby]["chrom_to_color"]
    run:
        print(params.colormap)
        # parse the printing information
        print_list = []
        for key in params.colormap:
            coord = key
            color = params.colormap[coord]
            scaf = coord.split(":")[0]
            pos_raw = coord.split(":")[1]
            if pos_raw == "all":
                pos_min = 1
                pos_max = 999999999
            else:
                pos_min = int(pos_raw.split("-")[0])
                pos_max = int(pos_raw.split("-")[1])
            print_list.append({"scaf": scaf, "pos_min": pos_min,
                               "pos_max": pos_max, "color": color})
        df = pd.DataFrame.from_dict(print_list)
        print(df)
        #df["colors_py"] =  df["color"].apply(matplotlib.colors.to_rgba)
        out_handle = open(output.prot_to_color, "w")
        xstruct = generate_coord_structs_from_chrom_to_loc(input.x_prot_to_loc)
        for prot in list(xstruct["prot_to_scaf"].keys()):
            scaf = xstruct["prot_to_scaf"][prot]
            if scaf in list(df["scaf"]):
                pos  = xstruct["prot_to_middle"][prot]
                query = "scaf == '{}' & pos_min <= {} & pos_max >= {}".format(scaf, pos, pos)
                color = df.query(query)["color"].values[0]
                print("{}\t{}".format(prot, color), file=out_handle)
            else:
                color = "#000000"
                print("{}\t{}".format(prot, color), file=out_handle)
        out_handle.close()
